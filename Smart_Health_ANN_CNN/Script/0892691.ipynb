{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0892691.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqxV5c7zbkfo",
        "colab_type": "text"
      },
      "source": [
        "Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCOe-qyvUF2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt\n",
        "import ast\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PZVLBrjcGJ4",
        "colab_type": "text"
      },
      "source": [
        "Read the Data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0u8cpDnUXAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "abf65378-75eb-4835-88b7-fe9d34eb3e5c"
      },
      "source": [
        "data_array=[]\n",
        "for file in range(1000):\n",
        "  \n",
        "  print(file,end=\" \")\n",
        "  #read the csv and store it as dataframe\n",
        "  data=pd.read_csv(\"/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/DS/data\"+str(file)+\".csv\",header=None)\n",
        "\n",
        "  #data is a dataframe and at index 0 we have the target value i.e optimizized value\n",
        "  target=float((data.iloc[0])[0].split(\":\")[1])\n",
        "\n",
        "  #Concatenating the remaining data in the file(except the target) in to string\n",
        "  #Generating a 50*50 matrix in string format\n",
        "  matrix=data.iloc[1:][0]\n",
        "  data_str=\"\"\n",
        "  for items in matrix.iteritems():\n",
        "    data_str=data_str+items[1]\n",
        "  \n",
        "  #Divide the string in to list using the delimiter ]. \n",
        "  #what you have now is a list of strings \n",
        "  #example l[0]= first row of 50*50 matrix stored as string\n",
        "  l=data_str[1:-1].split(\"]\")\n",
        "  \n",
        "  #final_matrix stores the entire 50*50 matrix as a single row (for each file)\n",
        "  final_matrix=[]\n",
        "  \n",
        "  #iterate over each string row of the 50*50 matrix\n",
        "  #replace the space with ',' and\n",
        "  #covert the string row to actual list\n",
        "  for x in l:\n",
        "    if(x!=''):\n",
        "      # replace the spaces between the number in the string with ','\n",
        "      temp=re.sub(r'(\\s+)',r',',x.strip()[1:].strip())\n",
        "      # Convert the string into a actual list and add it to final_matrix\n",
        "      final_matrix=final_matrix + (ast.literal_eval(\"[\"+temp+\"]\"))\n",
        "\n",
        "  #attach the target to the row\n",
        "  final_matrix.append(target)\n",
        "\n",
        "  #Data_array contains the complete table of dimension 1000*2501 (+1 for target)\n",
        "  data_array.append(final_matrix)\n"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0RTs76Q_yXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataframe \n",
        "df = pd.DataFrame(data_array)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne875cK-5CnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8071abac-3ef3-48d1-ada5-5636acdd126d"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owWt8YfiBL52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "e46700de-720a-4f79-92e7-6e005011c545"
      },
      "source": [
        "#display the top ten rows of the dataframe\n",
        "df.head(10)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156</td>\n",
              "      <td>499</td>\n",
              "      <td>284</td>\n",
              "      <td>25</td>\n",
              "      <td>300</td>\n",
              "      <td>40</td>\n",
              "      <td>346</td>\n",
              "      <td>108</td>\n",
              "      <td>190</td>\n",
              "      <td>458</td>\n",
              "      <td>358</td>\n",
              "      <td>320</td>\n",
              "      <td>124</td>\n",
              "      <td>93</td>\n",
              "      <td>283</td>\n",
              "      <td>103</td>\n",
              "      <td>64</td>\n",
              "      <td>273</td>\n",
              "      <td>449</td>\n",
              "      <td>225</td>\n",
              "      <td>487</td>\n",
              "      <td>54</td>\n",
              "      <td>497</td>\n",
              "      <td>420</td>\n",
              "      <td>207</td>\n",
              "      <td>204</td>\n",
              "      <td>175</td>\n",
              "      <td>423</td>\n",
              "      <td>301</td>\n",
              "      <td>20</td>\n",
              "      <td>306</td>\n",
              "      <td>139</td>\n",
              "      <td>354</td>\n",
              "      <td>36</td>\n",
              "      <td>326</td>\n",
              "      <td>79</td>\n",
              "      <td>345</td>\n",
              "      <td>435</td>\n",
              "      <td>295</td>\n",
              "      <td>443</td>\n",
              "      <td>...</td>\n",
              "      <td>316</td>\n",
              "      <td>30</td>\n",
              "      <td>236</td>\n",
              "      <td>293</td>\n",
              "      <td>230</td>\n",
              "      <td>205</td>\n",
              "      <td>484</td>\n",
              "      <td>77</td>\n",
              "      <td>89</td>\n",
              "      <td>366</td>\n",
              "      <td>240</td>\n",
              "      <td>419</td>\n",
              "      <td>225</td>\n",
              "      <td>325</td>\n",
              "      <td>136</td>\n",
              "      <td>214</td>\n",
              "      <td>24</td>\n",
              "      <td>85</td>\n",
              "      <td>472</td>\n",
              "      <td>478</td>\n",
              "      <td>470</td>\n",
              "      <td>371</td>\n",
              "      <td>154</td>\n",
              "      <td>410</td>\n",
              "      <td>374</td>\n",
              "      <td>94</td>\n",
              "      <td>395</td>\n",
              "      <td>468</td>\n",
              "      <td>25</td>\n",
              "      <td>315</td>\n",
              "      <td>348</td>\n",
              "      <td>312</td>\n",
              "      <td>420</td>\n",
              "      <td>295</td>\n",
              "      <td>327</td>\n",
              "      <td>68</td>\n",
              "      <td>271</td>\n",
              "      <td>302</td>\n",
              "      <td>47</td>\n",
              "      <td>1606.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84</td>\n",
              "      <td>46</td>\n",
              "      <td>272</td>\n",
              "      <td>52</td>\n",
              "      <td>329</td>\n",
              "      <td>217</td>\n",
              "      <td>387</td>\n",
              "      <td>107</td>\n",
              "      <td>337</td>\n",
              "      <td>69</td>\n",
              "      <td>199</td>\n",
              "      <td>392</td>\n",
              "      <td>483</td>\n",
              "      <td>153</td>\n",
              "      <td>279</td>\n",
              "      <td>356</td>\n",
              "      <td>322</td>\n",
              "      <td>153</td>\n",
              "      <td>119</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>175</td>\n",
              "      <td>202</td>\n",
              "      <td>402</td>\n",
              "      <td>401</td>\n",
              "      <td>426</td>\n",
              "      <td>68</td>\n",
              "      <td>289</td>\n",
              "      <td>338</td>\n",
              "      <td>260</td>\n",
              "      <td>390</td>\n",
              "      <td>22</td>\n",
              "      <td>292</td>\n",
              "      <td>258</td>\n",
              "      <td>347</td>\n",
              "      <td>371</td>\n",
              "      <td>89</td>\n",
              "      <td>26</td>\n",
              "      <td>451</td>\n",
              "      <td>444</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>283</td>\n",
              "      <td>393</td>\n",
              "      <td>158</td>\n",
              "      <td>168</td>\n",
              "      <td>252</td>\n",
              "      <td>359</td>\n",
              "      <td>145</td>\n",
              "      <td>298</td>\n",
              "      <td>285</td>\n",
              "      <td>422</td>\n",
              "      <td>424</td>\n",
              "      <td>251</td>\n",
              "      <td>75</td>\n",
              "      <td>379</td>\n",
              "      <td>250</td>\n",
              "      <td>215</td>\n",
              "      <td>238</td>\n",
              "      <td>346</td>\n",
              "      <td>134</td>\n",
              "      <td>362</td>\n",
              "      <td>217</td>\n",
              "      <td>402</td>\n",
              "      <td>474</td>\n",
              "      <td>410</td>\n",
              "      <td>264</td>\n",
              "      <td>142</td>\n",
              "      <td>120</td>\n",
              "      <td>157</td>\n",
              "      <td>444</td>\n",
              "      <td>22</td>\n",
              "      <td>417</td>\n",
              "      <td>390</td>\n",
              "      <td>66</td>\n",
              "      <td>249</td>\n",
              "      <td>285</td>\n",
              "      <td>153</td>\n",
              "      <td>494</td>\n",
              "      <td>223</td>\n",
              "      <td>1714.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>68</td>\n",
              "      <td>400</td>\n",
              "      <td>158</td>\n",
              "      <td>354</td>\n",
              "      <td>390</td>\n",
              "      <td>469</td>\n",
              "      <td>463</td>\n",
              "      <td>291</td>\n",
              "      <td>148</td>\n",
              "      <td>239</td>\n",
              "      <td>377</td>\n",
              "      <td>293</td>\n",
              "      <td>39</td>\n",
              "      <td>287</td>\n",
              "      <td>463</td>\n",
              "      <td>478</td>\n",
              "      <td>498</td>\n",
              "      <td>50</td>\n",
              "      <td>207</td>\n",
              "      <td>337</td>\n",
              "      <td>152</td>\n",
              "      <td>327</td>\n",
              "      <td>348</td>\n",
              "      <td>185</td>\n",
              "      <td>200</td>\n",
              "      <td>150</td>\n",
              "      <td>368</td>\n",
              "      <td>448</td>\n",
              "      <td>491</td>\n",
              "      <td>220</td>\n",
              "      <td>414</td>\n",
              "      <td>274</td>\n",
              "      <td>142</td>\n",
              "      <td>375</td>\n",
              "      <td>24</td>\n",
              "      <td>275</td>\n",
              "      <td>302</td>\n",
              "      <td>467</td>\n",
              "      <td>387</td>\n",
              "      <td>164</td>\n",
              "      <td>...</td>\n",
              "      <td>113</td>\n",
              "      <td>470</td>\n",
              "      <td>444</td>\n",
              "      <td>246</td>\n",
              "      <td>137</td>\n",
              "      <td>117</td>\n",
              "      <td>235</td>\n",
              "      <td>205</td>\n",
              "      <td>157</td>\n",
              "      <td>65</td>\n",
              "      <td>320</td>\n",
              "      <td>140</td>\n",
              "      <td>413</td>\n",
              "      <td>401</td>\n",
              "      <td>25</td>\n",
              "      <td>210</td>\n",
              "      <td>245</td>\n",
              "      <td>80</td>\n",
              "      <td>420</td>\n",
              "      <td>155</td>\n",
              "      <td>130</td>\n",
              "      <td>306</td>\n",
              "      <td>486</td>\n",
              "      <td>288</td>\n",
              "      <td>309</td>\n",
              "      <td>463</td>\n",
              "      <td>471</td>\n",
              "      <td>258</td>\n",
              "      <td>147</td>\n",
              "      <td>206</td>\n",
              "      <td>69</td>\n",
              "      <td>133</td>\n",
              "      <td>23</td>\n",
              "      <td>491</td>\n",
              "      <td>107</td>\n",
              "      <td>298</td>\n",
              "      <td>232</td>\n",
              "      <td>181</td>\n",
              "      <td>403</td>\n",
              "      <td>1936.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>400</td>\n",
              "      <td>148</td>\n",
              "      <td>24</td>\n",
              "      <td>118</td>\n",
              "      <td>468</td>\n",
              "      <td>343</td>\n",
              "      <td>191</td>\n",
              "      <td>436</td>\n",
              "      <td>185</td>\n",
              "      <td>251</td>\n",
              "      <td>458</td>\n",
              "      <td>73</td>\n",
              "      <td>201</td>\n",
              "      <td>387</td>\n",
              "      <td>162</td>\n",
              "      <td>375</td>\n",
              "      <td>369</td>\n",
              "      <td>222</td>\n",
              "      <td>263</td>\n",
              "      <td>35</td>\n",
              "      <td>456</td>\n",
              "      <td>456</td>\n",
              "      <td>175</td>\n",
              "      <td>428</td>\n",
              "      <td>23</td>\n",
              "      <td>439</td>\n",
              "      <td>209</td>\n",
              "      <td>177</td>\n",
              "      <td>213</td>\n",
              "      <td>337</td>\n",
              "      <td>279</td>\n",
              "      <td>427</td>\n",
              "      <td>144</td>\n",
              "      <td>60</td>\n",
              "      <td>393</td>\n",
              "      <td>342</td>\n",
              "      <td>64</td>\n",
              "      <td>23</td>\n",
              "      <td>447</td>\n",
              "      <td>81</td>\n",
              "      <td>...</td>\n",
              "      <td>29</td>\n",
              "      <td>460</td>\n",
              "      <td>427</td>\n",
              "      <td>319</td>\n",
              "      <td>286</td>\n",
              "      <td>238</td>\n",
              "      <td>149</td>\n",
              "      <td>314</td>\n",
              "      <td>426</td>\n",
              "      <td>157</td>\n",
              "      <td>382</td>\n",
              "      <td>443</td>\n",
              "      <td>140</td>\n",
              "      <td>91</td>\n",
              "      <td>424</td>\n",
              "      <td>496</td>\n",
              "      <td>452</td>\n",
              "      <td>470</td>\n",
              "      <td>78</td>\n",
              "      <td>59</td>\n",
              "      <td>109</td>\n",
              "      <td>229</td>\n",
              "      <td>49</td>\n",
              "      <td>472</td>\n",
              "      <td>465</td>\n",
              "      <td>425</td>\n",
              "      <td>122</td>\n",
              "      <td>203</td>\n",
              "      <td>207</td>\n",
              "      <td>61</td>\n",
              "      <td>279</td>\n",
              "      <td>47</td>\n",
              "      <td>91</td>\n",
              "      <td>71</td>\n",
              "      <td>115</td>\n",
              "      <td>62</td>\n",
              "      <td>176</td>\n",
              "      <td>400</td>\n",
              "      <td>455</td>\n",
              "      <td>1811.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>281</td>\n",
              "      <td>161</td>\n",
              "      <td>226</td>\n",
              "      <td>67</td>\n",
              "      <td>77</td>\n",
              "      <td>114</td>\n",
              "      <td>217</td>\n",
              "      <td>421</td>\n",
              "      <td>74</td>\n",
              "      <td>490</td>\n",
              "      <td>237</td>\n",
              "      <td>268</td>\n",
              "      <td>305</td>\n",
              "      <td>138</td>\n",
              "      <td>85</td>\n",
              "      <td>38</td>\n",
              "      <td>281</td>\n",
              "      <td>401</td>\n",
              "      <td>380</td>\n",
              "      <td>226</td>\n",
              "      <td>382</td>\n",
              "      <td>52</td>\n",
              "      <td>163</td>\n",
              "      <td>353</td>\n",
              "      <td>388</td>\n",
              "      <td>350</td>\n",
              "      <td>194</td>\n",
              "      <td>42</td>\n",
              "      <td>446</td>\n",
              "      <td>403</td>\n",
              "      <td>344</td>\n",
              "      <td>189</td>\n",
              "      <td>131</td>\n",
              "      <td>363</td>\n",
              "      <td>185</td>\n",
              "      <td>164</td>\n",
              "      <td>291</td>\n",
              "      <td>375</td>\n",
              "      <td>126</td>\n",
              "      <td>...</td>\n",
              "      <td>207</td>\n",
              "      <td>114</td>\n",
              "      <td>191</td>\n",
              "      <td>57</td>\n",
              "      <td>299</td>\n",
              "      <td>109</td>\n",
              "      <td>28</td>\n",
              "      <td>310</td>\n",
              "      <td>405</td>\n",
              "      <td>455</td>\n",
              "      <td>431</td>\n",
              "      <td>219</td>\n",
              "      <td>469</td>\n",
              "      <td>266</td>\n",
              "      <td>26</td>\n",
              "      <td>38</td>\n",
              "      <td>422</td>\n",
              "      <td>380</td>\n",
              "      <td>477</td>\n",
              "      <td>318</td>\n",
              "      <td>426</td>\n",
              "      <td>249</td>\n",
              "      <td>158</td>\n",
              "      <td>189</td>\n",
              "      <td>483</td>\n",
              "      <td>477</td>\n",
              "      <td>417</td>\n",
              "      <td>440</td>\n",
              "      <td>21</td>\n",
              "      <td>169</td>\n",
              "      <td>448</td>\n",
              "      <td>247</td>\n",
              "      <td>51</td>\n",
              "      <td>285</td>\n",
              "      <td>162</td>\n",
              "      <td>397</td>\n",
              "      <td>61</td>\n",
              "      <td>374</td>\n",
              "      <td>206</td>\n",
              "      <td>1890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>413</td>\n",
              "      <td>430</td>\n",
              "      <td>451</td>\n",
              "      <td>469</td>\n",
              "      <td>122</td>\n",
              "      <td>445</td>\n",
              "      <td>88</td>\n",
              "      <td>159</td>\n",
              "      <td>102</td>\n",
              "      <td>283</td>\n",
              "      <td>403</td>\n",
              "      <td>353</td>\n",
              "      <td>308</td>\n",
              "      <td>214</td>\n",
              "      <td>204</td>\n",
              "      <td>56</td>\n",
              "      <td>40</td>\n",
              "      <td>153</td>\n",
              "      <td>85</td>\n",
              "      <td>74</td>\n",
              "      <td>93</td>\n",
              "      <td>209</td>\n",
              "      <td>133</td>\n",
              "      <td>159</td>\n",
              "      <td>142</td>\n",
              "      <td>294</td>\n",
              "      <td>248</td>\n",
              "      <td>124</td>\n",
              "      <td>329</td>\n",
              "      <td>447</td>\n",
              "      <td>52</td>\n",
              "      <td>121</td>\n",
              "      <td>29</td>\n",
              "      <td>296</td>\n",
              "      <td>43</td>\n",
              "      <td>253</td>\n",
              "      <td>437</td>\n",
              "      <td>253</td>\n",
              "      <td>160</td>\n",
              "      <td>68</td>\n",
              "      <td>...</td>\n",
              "      <td>254</td>\n",
              "      <td>62</td>\n",
              "      <td>486</td>\n",
              "      <td>188</td>\n",
              "      <td>103</td>\n",
              "      <td>460</td>\n",
              "      <td>118</td>\n",
              "      <td>422</td>\n",
              "      <td>213</td>\n",
              "      <td>96</td>\n",
              "      <td>218</td>\n",
              "      <td>351</td>\n",
              "      <td>187</td>\n",
              "      <td>300</td>\n",
              "      <td>385</td>\n",
              "      <td>271</td>\n",
              "      <td>449</td>\n",
              "      <td>400</td>\n",
              "      <td>237</td>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>120</td>\n",
              "      <td>350</td>\n",
              "      <td>417</td>\n",
              "      <td>142</td>\n",
              "      <td>470</td>\n",
              "      <td>367</td>\n",
              "      <td>180</td>\n",
              "      <td>356</td>\n",
              "      <td>173</td>\n",
              "      <td>346</td>\n",
              "      <td>174</td>\n",
              "      <td>164</td>\n",
              "      <td>455</td>\n",
              "      <td>106</td>\n",
              "      <td>300</td>\n",
              "      <td>191</td>\n",
              "      <td>466</td>\n",
              "      <td>60</td>\n",
              "      <td>1937.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>221</td>\n",
              "      <td>106</td>\n",
              "      <td>390</td>\n",
              "      <td>268</td>\n",
              "      <td>146</td>\n",
              "      <td>95</td>\n",
              "      <td>289</td>\n",
              "      <td>272</td>\n",
              "      <td>273</td>\n",
              "      <td>61</td>\n",
              "      <td>326</td>\n",
              "      <td>247</td>\n",
              "      <td>264</td>\n",
              "      <td>378</td>\n",
              "      <td>254</td>\n",
              "      <td>420</td>\n",
              "      <td>72</td>\n",
              "      <td>283</td>\n",
              "      <td>326</td>\n",
              "      <td>81</td>\n",
              "      <td>248</td>\n",
              "      <td>132</td>\n",
              "      <td>302</td>\n",
              "      <td>23</td>\n",
              "      <td>493</td>\n",
              "      <td>241</td>\n",
              "      <td>491</td>\n",
              "      <td>356</td>\n",
              "      <td>80</td>\n",
              "      <td>436</td>\n",
              "      <td>289</td>\n",
              "      <td>302</td>\n",
              "      <td>329</td>\n",
              "      <td>174</td>\n",
              "      <td>424</td>\n",
              "      <td>113</td>\n",
              "      <td>110</td>\n",
              "      <td>382</td>\n",
              "      <td>193</td>\n",
              "      <td>78</td>\n",
              "      <td>...</td>\n",
              "      <td>429</td>\n",
              "      <td>461</td>\n",
              "      <td>257</td>\n",
              "      <td>453</td>\n",
              "      <td>268</td>\n",
              "      <td>86</td>\n",
              "      <td>243</td>\n",
              "      <td>65</td>\n",
              "      <td>201</td>\n",
              "      <td>314</td>\n",
              "      <td>165</td>\n",
              "      <td>288</td>\n",
              "      <td>351</td>\n",
              "      <td>81</td>\n",
              "      <td>137</td>\n",
              "      <td>394</td>\n",
              "      <td>127</td>\n",
              "      <td>88</td>\n",
              "      <td>212</td>\n",
              "      <td>239</td>\n",
              "      <td>115</td>\n",
              "      <td>24</td>\n",
              "      <td>430</td>\n",
              "      <td>469</td>\n",
              "      <td>244</td>\n",
              "      <td>113</td>\n",
              "      <td>321</td>\n",
              "      <td>241</td>\n",
              "      <td>478</td>\n",
              "      <td>193</td>\n",
              "      <td>317</td>\n",
              "      <td>59</td>\n",
              "      <td>289</td>\n",
              "      <td>499</td>\n",
              "      <td>54</td>\n",
              "      <td>114</td>\n",
              "      <td>379</td>\n",
              "      <td>218</td>\n",
              "      <td>391</td>\n",
              "      <td>1713.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30</td>\n",
              "      <td>250</td>\n",
              "      <td>155</td>\n",
              "      <td>390</td>\n",
              "      <td>171</td>\n",
              "      <td>455</td>\n",
              "      <td>479</td>\n",
              "      <td>156</td>\n",
              "      <td>47</td>\n",
              "      <td>330</td>\n",
              "      <td>440</td>\n",
              "      <td>238</td>\n",
              "      <td>345</td>\n",
              "      <td>261</td>\n",
              "      <td>281</td>\n",
              "      <td>45</td>\n",
              "      <td>266</td>\n",
              "      <td>320</td>\n",
              "      <td>444</td>\n",
              "      <td>33</td>\n",
              "      <td>270</td>\n",
              "      <td>106</td>\n",
              "      <td>193</td>\n",
              "      <td>263</td>\n",
              "      <td>323</td>\n",
              "      <td>41</td>\n",
              "      <td>308</td>\n",
              "      <td>269</td>\n",
              "      <td>120</td>\n",
              "      <td>47</td>\n",
              "      <td>138</td>\n",
              "      <td>400</td>\n",
              "      <td>464</td>\n",
              "      <td>231</td>\n",
              "      <td>121</td>\n",
              "      <td>305</td>\n",
              "      <td>104</td>\n",
              "      <td>289</td>\n",
              "      <td>139</td>\n",
              "      <td>44</td>\n",
              "      <td>...</td>\n",
              "      <td>459</td>\n",
              "      <td>365</td>\n",
              "      <td>146</td>\n",
              "      <td>290</td>\n",
              "      <td>423</td>\n",
              "      <td>174</td>\n",
              "      <td>264</td>\n",
              "      <td>280</td>\n",
              "      <td>431</td>\n",
              "      <td>482</td>\n",
              "      <td>111</td>\n",
              "      <td>276</td>\n",
              "      <td>126</td>\n",
              "      <td>452</td>\n",
              "      <td>247</td>\n",
              "      <td>154</td>\n",
              "      <td>61</td>\n",
              "      <td>449</td>\n",
              "      <td>303</td>\n",
              "      <td>496</td>\n",
              "      <td>321</td>\n",
              "      <td>385</td>\n",
              "      <td>138</td>\n",
              "      <td>369</td>\n",
              "      <td>160</td>\n",
              "      <td>458</td>\n",
              "      <td>316</td>\n",
              "      <td>401</td>\n",
              "      <td>126</td>\n",
              "      <td>410</td>\n",
              "      <td>347</td>\n",
              "      <td>342</td>\n",
              "      <td>81</td>\n",
              "      <td>459</td>\n",
              "      <td>378</td>\n",
              "      <td>332</td>\n",
              "      <td>116</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1721.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>410</td>\n",
              "      <td>215</td>\n",
              "      <td>84</td>\n",
              "      <td>277</td>\n",
              "      <td>378</td>\n",
              "      <td>23</td>\n",
              "      <td>180</td>\n",
              "      <td>234</td>\n",
              "      <td>109</td>\n",
              "      <td>171</td>\n",
              "      <td>268</td>\n",
              "      <td>307</td>\n",
              "      <td>404</td>\n",
              "      <td>349</td>\n",
              "      <td>197</td>\n",
              "      <td>44</td>\n",
              "      <td>282</td>\n",
              "      <td>129</td>\n",
              "      <td>170</td>\n",
              "      <td>195</td>\n",
              "      <td>118</td>\n",
              "      <td>80</td>\n",
              "      <td>244</td>\n",
              "      <td>114</td>\n",
              "      <td>179</td>\n",
              "      <td>495</td>\n",
              "      <td>119</td>\n",
              "      <td>65</td>\n",
              "      <td>417</td>\n",
              "      <td>499</td>\n",
              "      <td>295</td>\n",
              "      <td>298</td>\n",
              "      <td>450</td>\n",
              "      <td>482</td>\n",
              "      <td>58</td>\n",
              "      <td>427</td>\n",
              "      <td>86</td>\n",
              "      <td>355</td>\n",
              "      <td>180</td>\n",
              "      <td>61</td>\n",
              "      <td>...</td>\n",
              "      <td>184</td>\n",
              "      <td>75</td>\n",
              "      <td>315</td>\n",
              "      <td>260</td>\n",
              "      <td>383</td>\n",
              "      <td>59</td>\n",
              "      <td>480</td>\n",
              "      <td>212</td>\n",
              "      <td>71</td>\n",
              "      <td>357</td>\n",
              "      <td>425</td>\n",
              "      <td>57</td>\n",
              "      <td>47</td>\n",
              "      <td>439</td>\n",
              "      <td>382</td>\n",
              "      <td>83</td>\n",
              "      <td>294</td>\n",
              "      <td>75</td>\n",
              "      <td>48</td>\n",
              "      <td>354</td>\n",
              "      <td>296</td>\n",
              "      <td>251</td>\n",
              "      <td>368</td>\n",
              "      <td>461</td>\n",
              "      <td>236</td>\n",
              "      <td>252</td>\n",
              "      <td>347</td>\n",
              "      <td>137</td>\n",
              "      <td>371</td>\n",
              "      <td>25</td>\n",
              "      <td>294</td>\n",
              "      <td>57</td>\n",
              "      <td>135</td>\n",
              "      <td>259</td>\n",
              "      <td>324</td>\n",
              "      <td>230</td>\n",
              "      <td>445</td>\n",
              "      <td>297</td>\n",
              "      <td>288</td>\n",
              "      <td>1706.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>90</td>\n",
              "      <td>394</td>\n",
              "      <td>498</td>\n",
              "      <td>350</td>\n",
              "      <td>236</td>\n",
              "      <td>281</td>\n",
              "      <td>123</td>\n",
              "      <td>138</td>\n",
              "      <td>20</td>\n",
              "      <td>487</td>\n",
              "      <td>216</td>\n",
              "      <td>148</td>\n",
              "      <td>454</td>\n",
              "      <td>128</td>\n",
              "      <td>327</td>\n",
              "      <td>178</td>\n",
              "      <td>294</td>\n",
              "      <td>159</td>\n",
              "      <td>69</td>\n",
              "      <td>386</td>\n",
              "      <td>431</td>\n",
              "      <td>470</td>\n",
              "      <td>397</td>\n",
              "      <td>296</td>\n",
              "      <td>487</td>\n",
              "      <td>344</td>\n",
              "      <td>420</td>\n",
              "      <td>302</td>\n",
              "      <td>209</td>\n",
              "      <td>167</td>\n",
              "      <td>489</td>\n",
              "      <td>211</td>\n",
              "      <td>363</td>\n",
              "      <td>168</td>\n",
              "      <td>230</td>\n",
              "      <td>442</td>\n",
              "      <td>315</td>\n",
              "      <td>195</td>\n",
              "      <td>112</td>\n",
              "      <td>378</td>\n",
              "      <td>...</td>\n",
              "      <td>116</td>\n",
              "      <td>264</td>\n",
              "      <td>223</td>\n",
              "      <td>126</td>\n",
              "      <td>478</td>\n",
              "      <td>197</td>\n",
              "      <td>325</td>\n",
              "      <td>318</td>\n",
              "      <td>75</td>\n",
              "      <td>99</td>\n",
              "      <td>384</td>\n",
              "      <td>435</td>\n",
              "      <td>328</td>\n",
              "      <td>420</td>\n",
              "      <td>137</td>\n",
              "      <td>172</td>\n",
              "      <td>57</td>\n",
              "      <td>314</td>\n",
              "      <td>183</td>\n",
              "      <td>100</td>\n",
              "      <td>436</td>\n",
              "      <td>410</td>\n",
              "      <td>23</td>\n",
              "      <td>380</td>\n",
              "      <td>397</td>\n",
              "      <td>256</td>\n",
              "      <td>236</td>\n",
              "      <td>101</td>\n",
              "      <td>304</td>\n",
              "      <td>156</td>\n",
              "      <td>61</td>\n",
              "      <td>394</td>\n",
              "      <td>413</td>\n",
              "      <td>130</td>\n",
              "      <td>387</td>\n",
              "      <td>328</td>\n",
              "      <td>460</td>\n",
              "      <td>141</td>\n",
              "      <td>432</td>\n",
              "      <td>1775.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     ...  2495  2496  2497  2498  2499    2500\n",
              "0   156   499   284    25   300    40  ...   327    68   271   302    47  1606.0\n",
              "1    84    46   272    52   329   217  ...   249   285   153   494   223  1714.0\n",
              "2    68   400   158   354   390   469  ...   107   298   232   181   403  1936.0\n",
              "3   400   148    24   118   468   343  ...   115    62   176   400   455  1811.0\n",
              "4    28   281   161   226    67    77  ...   162   397    61   374   206  1890.0\n",
              "5   413   430   451   469   122   445  ...   106   300   191   466    60  1937.0\n",
              "6   221   106   390   268   146    95  ...    54   114   379   218   391  1713.0\n",
              "7    30   250   155   390   171   455  ...   378   332   116    45    45  1721.0\n",
              "8   410   215    84   277   378    23  ...   324   230   445   297   288  1706.0\n",
              "9    90   394   498   350   236   281  ...   387   328   460   141   432  1775.0\n",
              "\n",
              "[10 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKutKYsRBL3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "fb4a552c-0874-43ba-ee80-4b203a54b85a"
      },
      "source": [
        "sns.distplot(df[2500]);"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDm1gc_CcLid",
        "colab_type": "text"
      },
      "source": [
        "Scale the Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkbbT_2RVf6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get all the columns of the dataframe\n",
        "columnsToScale=list(df.columns)\n",
        "#ignore the last column as it is the target\n",
        "columnsToScale=columnsToScale[:-1]\n",
        "#Define the MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "#Scale the features in the range of 0-1\n",
        "df[columnsToScale] = scaler.fit_transform(df[columnsToScale])"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXiLJ9JBLyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "dbb2f55f-2735-4e21-d51c-4f20d4399053"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.284519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.551148</td>\n",
              "      <td>0.010438</td>\n",
              "      <td>0.583682</td>\n",
              "      <td>0.041754</td>\n",
              "      <td>0.680585</td>\n",
              "      <td>0.183716</td>\n",
              "      <td>0.354906</td>\n",
              "      <td>0.916318</td>\n",
              "      <td>0.705637</td>\n",
              "      <td>0.626305</td>\n",
              "      <td>0.217119</td>\n",
              "      <td>0.152401</td>\n",
              "      <td>0.548117</td>\n",
              "      <td>0.173278</td>\n",
              "      <td>0.091858</td>\n",
              "      <td>0.528184</td>\n",
              "      <td>0.897490</td>\n",
              "      <td>0.427975</td>\n",
              "      <td>0.974948</td>\n",
              "      <td>0.070981</td>\n",
              "      <td>0.995825</td>\n",
              "      <td>0.834728</td>\n",
              "      <td>0.390397</td>\n",
              "      <td>0.384134</td>\n",
              "      <td>0.323591</td>\n",
              "      <td>0.841336</td>\n",
              "      <td>0.585774</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.597077</td>\n",
              "      <td>0.248434</td>\n",
              "      <td>0.697286</td>\n",
              "      <td>0.033403</td>\n",
              "      <td>0.638831</td>\n",
              "      <td>0.123173</td>\n",
              "      <td>0.678497</td>\n",
              "      <td>0.866388</td>\n",
              "      <td>0.574113</td>\n",
              "      <td>0.883090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.617954</td>\n",
              "      <td>0.020921</td>\n",
              "      <td>0.450939</td>\n",
              "      <td>0.569937</td>\n",
              "      <td>0.439331</td>\n",
              "      <td>0.386221</td>\n",
              "      <td>0.968685</td>\n",
              "      <td>0.118998</td>\n",
              "      <td>0.144050</td>\n",
              "      <td>0.722338</td>\n",
              "      <td>0.459290</td>\n",
              "      <td>0.832985</td>\n",
              "      <td>0.427975</td>\n",
              "      <td>0.636743</td>\n",
              "      <td>0.242171</td>\n",
              "      <td>0.405010</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.135699</td>\n",
              "      <td>0.943633</td>\n",
              "      <td>0.956159</td>\n",
              "      <td>0.939457</td>\n",
              "      <td>0.732777</td>\n",
              "      <td>0.279749</td>\n",
              "      <td>0.814196</td>\n",
              "      <td>0.739040</td>\n",
              "      <td>0.154812</td>\n",
              "      <td>0.782881</td>\n",
              "      <td>0.935282</td>\n",
              "      <td>0.010438</td>\n",
              "      <td>0.615866</td>\n",
              "      <td>0.684760</td>\n",
              "      <td>0.608787</td>\n",
              "      <td>0.835073</td>\n",
              "      <td>0.574113</td>\n",
              "      <td>0.640919</td>\n",
              "      <td>0.100209</td>\n",
              "      <td>0.524008</td>\n",
              "      <td>0.588727</td>\n",
              "      <td>0.056367</td>\n",
              "      <td>1606.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.133891</td>\n",
              "      <td>0.054280</td>\n",
              "      <td>0.526096</td>\n",
              "      <td>0.066806</td>\n",
              "      <td>0.644351</td>\n",
              "      <td>0.411273</td>\n",
              "      <td>0.766180</td>\n",
              "      <td>0.181628</td>\n",
              "      <td>0.661795</td>\n",
              "      <td>0.102510</td>\n",
              "      <td>0.373695</td>\n",
              "      <td>0.776618</td>\n",
              "      <td>0.966597</td>\n",
              "      <td>0.277662</td>\n",
              "      <td>0.539749</td>\n",
              "      <td>0.701461</td>\n",
              "      <td>0.630480</td>\n",
              "      <td>0.277662</td>\n",
              "      <td>0.207113</td>\n",
              "      <td>0.167015</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>0.323591</td>\n",
              "      <td>0.379958</td>\n",
              "      <td>0.797071</td>\n",
              "      <td>0.795407</td>\n",
              "      <td>0.847599</td>\n",
              "      <td>0.100209</td>\n",
              "      <td>0.561587</td>\n",
              "      <td>0.663180</td>\n",
              "      <td>0.501044</td>\n",
              "      <td>0.772443</td>\n",
              "      <td>0.004175</td>\n",
              "      <td>0.567850</td>\n",
              "      <td>0.496868</td>\n",
              "      <td>0.682672</td>\n",
              "      <td>0.732777</td>\n",
              "      <td>0.144050</td>\n",
              "      <td>0.012526</td>\n",
              "      <td>0.899791</td>\n",
              "      <td>0.885177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025052</td>\n",
              "      <td>0.550209</td>\n",
              "      <td>0.778706</td>\n",
              "      <td>0.288100</td>\n",
              "      <td>0.309623</td>\n",
              "      <td>0.484342</td>\n",
              "      <td>0.707724</td>\n",
              "      <td>0.260960</td>\n",
              "      <td>0.580376</td>\n",
              "      <td>0.553236</td>\n",
              "      <td>0.839248</td>\n",
              "      <td>0.843424</td>\n",
              "      <td>0.482255</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>0.749478</td>\n",
              "      <td>0.480167</td>\n",
              "      <td>0.407098</td>\n",
              "      <td>0.455115</td>\n",
              "      <td>0.680585</td>\n",
              "      <td>0.237996</td>\n",
              "      <td>0.713987</td>\n",
              "      <td>0.411273</td>\n",
              "      <td>0.797495</td>\n",
              "      <td>0.947808</td>\n",
              "      <td>0.814196</td>\n",
              "      <td>0.510460</td>\n",
              "      <td>0.254697</td>\n",
              "      <td>0.208768</td>\n",
              "      <td>0.286013</td>\n",
              "      <td>0.885177</td>\n",
              "      <td>0.004175</td>\n",
              "      <td>0.828452</td>\n",
              "      <td>0.772443</td>\n",
              "      <td>0.096033</td>\n",
              "      <td>0.478079</td>\n",
              "      <td>0.553236</td>\n",
              "      <td>0.277662</td>\n",
              "      <td>0.989562</td>\n",
              "      <td>0.423800</td>\n",
              "      <td>1714.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.100418</td>\n",
              "      <td>0.793319</td>\n",
              "      <td>0.288100</td>\n",
              "      <td>0.697286</td>\n",
              "      <td>0.771967</td>\n",
              "      <td>0.937370</td>\n",
              "      <td>0.924843</td>\n",
              "      <td>0.565762</td>\n",
              "      <td>0.267223</td>\n",
              "      <td>0.458159</td>\n",
              "      <td>0.745303</td>\n",
              "      <td>0.569937</td>\n",
              "      <td>0.039666</td>\n",
              "      <td>0.557411</td>\n",
              "      <td>0.924686</td>\n",
              "      <td>0.956159</td>\n",
              "      <td>0.997912</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.391213</td>\n",
              "      <td>0.661795</td>\n",
              "      <td>0.275574</td>\n",
              "      <td>0.640919</td>\n",
              "      <td>0.684760</td>\n",
              "      <td>0.343096</td>\n",
              "      <td>0.375783</td>\n",
              "      <td>0.271399</td>\n",
              "      <td>0.726514</td>\n",
              "      <td>0.893528</td>\n",
              "      <td>0.983264</td>\n",
              "      <td>0.417537</td>\n",
              "      <td>0.822547</td>\n",
              "      <td>0.530271</td>\n",
              "      <td>0.254697</td>\n",
              "      <td>0.741127</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.532359</td>\n",
              "      <td>0.588727</td>\n",
              "      <td>0.933194</td>\n",
              "      <td>0.766180</td>\n",
              "      <td>0.300626</td>\n",
              "      <td>...</td>\n",
              "      <td>0.194154</td>\n",
              "      <td>0.941423</td>\n",
              "      <td>0.885177</td>\n",
              "      <td>0.471816</td>\n",
              "      <td>0.244770</td>\n",
              "      <td>0.202505</td>\n",
              "      <td>0.448852</td>\n",
              "      <td>0.386221</td>\n",
              "      <td>0.286013</td>\n",
              "      <td>0.093946</td>\n",
              "      <td>0.626305</td>\n",
              "      <td>0.250522</td>\n",
              "      <td>0.820459</td>\n",
              "      <td>0.795407</td>\n",
              "      <td>0.010438</td>\n",
              "      <td>0.396660</td>\n",
              "      <td>0.469729</td>\n",
              "      <td>0.125261</td>\n",
              "      <td>0.835073</td>\n",
              "      <td>0.281837</td>\n",
              "      <td>0.229645</td>\n",
              "      <td>0.597077</td>\n",
              "      <td>0.972860</td>\n",
              "      <td>0.559499</td>\n",
              "      <td>0.603340</td>\n",
              "      <td>0.926778</td>\n",
              "      <td>0.941545</td>\n",
              "      <td>0.496868</td>\n",
              "      <td>0.265136</td>\n",
              "      <td>0.388309</td>\n",
              "      <td>0.102296</td>\n",
              "      <td>0.234310</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.983299</td>\n",
              "      <td>0.181628</td>\n",
              "      <td>0.580376</td>\n",
              "      <td>0.442589</td>\n",
              "      <td>0.336117</td>\n",
              "      <td>0.799582</td>\n",
              "      <td>1936.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.794979</td>\n",
              "      <td>0.267223</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.204593</td>\n",
              "      <td>0.935146</td>\n",
              "      <td>0.674322</td>\n",
              "      <td>0.356994</td>\n",
              "      <td>0.868476</td>\n",
              "      <td>0.344468</td>\n",
              "      <td>0.483264</td>\n",
              "      <td>0.914405</td>\n",
              "      <td>0.110647</td>\n",
              "      <td>0.377871</td>\n",
              "      <td>0.766180</td>\n",
              "      <td>0.294979</td>\n",
              "      <td>0.741127</td>\n",
              "      <td>0.728601</td>\n",
              "      <td>0.421712</td>\n",
              "      <td>0.508368</td>\n",
              "      <td>0.031315</td>\n",
              "      <td>0.910230</td>\n",
              "      <td>0.910230</td>\n",
              "      <td>0.323591</td>\n",
              "      <td>0.851464</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.874739</td>\n",
              "      <td>0.394572</td>\n",
              "      <td>0.327766</td>\n",
              "      <td>0.401674</td>\n",
              "      <td>0.661795</td>\n",
              "      <td>0.540710</td>\n",
              "      <td>0.849687</td>\n",
              "      <td>0.258873</td>\n",
              "      <td>0.083507</td>\n",
              "      <td>0.778706</td>\n",
              "      <td>0.672234</td>\n",
              "      <td>0.091858</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.891441</td>\n",
              "      <td>0.127349</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018789</td>\n",
              "      <td>0.920502</td>\n",
              "      <td>0.849687</td>\n",
              "      <td>0.624217</td>\n",
              "      <td>0.556485</td>\n",
              "      <td>0.455115</td>\n",
              "      <td>0.269311</td>\n",
              "      <td>0.613779</td>\n",
              "      <td>0.847599</td>\n",
              "      <td>0.286013</td>\n",
              "      <td>0.755741</td>\n",
              "      <td>0.883090</td>\n",
              "      <td>0.250522</td>\n",
              "      <td>0.148225</td>\n",
              "      <td>0.843424</td>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.901879</td>\n",
              "      <td>0.939457</td>\n",
              "      <td>0.121086</td>\n",
              "      <td>0.081420</td>\n",
              "      <td>0.185804</td>\n",
              "      <td>0.436326</td>\n",
              "      <td>0.060543</td>\n",
              "      <td>0.943633</td>\n",
              "      <td>0.929019</td>\n",
              "      <td>0.847280</td>\n",
              "      <td>0.212944</td>\n",
              "      <td>0.382046</td>\n",
              "      <td>0.390397</td>\n",
              "      <td>0.085595</td>\n",
              "      <td>0.540710</td>\n",
              "      <td>0.054393</td>\n",
              "      <td>0.148225</td>\n",
              "      <td>0.106472</td>\n",
              "      <td>0.198330</td>\n",
              "      <td>0.087683</td>\n",
              "      <td>0.325678</td>\n",
              "      <td>0.793319</td>\n",
              "      <td>0.908142</td>\n",
              "      <td>1811.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.016736</td>\n",
              "      <td>0.544885</td>\n",
              "      <td>0.294363</td>\n",
              "      <td>0.430063</td>\n",
              "      <td>0.096234</td>\n",
              "      <td>0.118998</td>\n",
              "      <td>0.196242</td>\n",
              "      <td>0.411273</td>\n",
              "      <td>0.837161</td>\n",
              "      <td>0.112971</td>\n",
              "      <td>0.981211</td>\n",
              "      <td>0.453027</td>\n",
              "      <td>0.517745</td>\n",
              "      <td>0.594990</td>\n",
              "      <td>0.244770</td>\n",
              "      <td>0.135699</td>\n",
              "      <td>0.037578</td>\n",
              "      <td>0.544885</td>\n",
              "      <td>0.797071</td>\n",
              "      <td>0.751566</td>\n",
              "      <td>0.430063</td>\n",
              "      <td>0.755741</td>\n",
              "      <td>0.066806</td>\n",
              "      <td>0.297071</td>\n",
              "      <td>0.695198</td>\n",
              "      <td>0.768267</td>\n",
              "      <td>0.688935</td>\n",
              "      <td>0.363257</td>\n",
              "      <td>0.043933</td>\n",
              "      <td>0.889353</td>\n",
              "      <td>0.799582</td>\n",
              "      <td>0.676409</td>\n",
              "      <td>0.352818</td>\n",
              "      <td>0.231733</td>\n",
              "      <td>0.716075</td>\n",
              "      <td>0.344468</td>\n",
              "      <td>0.300626</td>\n",
              "      <td>0.565762</td>\n",
              "      <td>0.741127</td>\n",
              "      <td>0.221294</td>\n",
              "      <td>...</td>\n",
              "      <td>0.390397</td>\n",
              "      <td>0.196653</td>\n",
              "      <td>0.356994</td>\n",
              "      <td>0.077244</td>\n",
              "      <td>0.583682</td>\n",
              "      <td>0.185804</td>\n",
              "      <td>0.016701</td>\n",
              "      <td>0.605428</td>\n",
              "      <td>0.803758</td>\n",
              "      <td>0.908142</td>\n",
              "      <td>0.858038</td>\n",
              "      <td>0.415449</td>\n",
              "      <td>0.937370</td>\n",
              "      <td>0.513570</td>\n",
              "      <td>0.012526</td>\n",
              "      <td>0.037578</td>\n",
              "      <td>0.839248</td>\n",
              "      <td>0.751566</td>\n",
              "      <td>0.954071</td>\n",
              "      <td>0.622129</td>\n",
              "      <td>0.847599</td>\n",
              "      <td>0.478079</td>\n",
              "      <td>0.288100</td>\n",
              "      <td>0.352818</td>\n",
              "      <td>0.966597</td>\n",
              "      <td>0.956067</td>\n",
              "      <td>0.828810</td>\n",
              "      <td>0.876827</td>\n",
              "      <td>0.002088</td>\n",
              "      <td>0.311065</td>\n",
              "      <td>0.893528</td>\n",
              "      <td>0.472803</td>\n",
              "      <td>0.064718</td>\n",
              "      <td>0.553236</td>\n",
              "      <td>0.296451</td>\n",
              "      <td>0.787056</td>\n",
              "      <td>0.085595</td>\n",
              "      <td>0.739040</td>\n",
              "      <td>0.388309</td>\n",
              "      <td>1890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.822176</td>\n",
              "      <td>0.855950</td>\n",
              "      <td>0.899791</td>\n",
              "      <td>0.937370</td>\n",
              "      <td>0.211297</td>\n",
              "      <td>0.887265</td>\n",
              "      <td>0.141962</td>\n",
              "      <td>0.290188</td>\n",
              "      <td>0.171190</td>\n",
              "      <td>0.550209</td>\n",
              "      <td>0.799582</td>\n",
              "      <td>0.695198</td>\n",
              "      <td>0.601253</td>\n",
              "      <td>0.405010</td>\n",
              "      <td>0.382845</td>\n",
              "      <td>0.075157</td>\n",
              "      <td>0.041754</td>\n",
              "      <td>0.277662</td>\n",
              "      <td>0.135983</td>\n",
              "      <td>0.112735</td>\n",
              "      <td>0.152401</td>\n",
              "      <td>0.394572</td>\n",
              "      <td>0.235908</td>\n",
              "      <td>0.288703</td>\n",
              "      <td>0.254697</td>\n",
              "      <td>0.572025</td>\n",
              "      <td>0.475992</td>\n",
              "      <td>0.217119</td>\n",
              "      <td>0.644351</td>\n",
              "      <td>0.891441</td>\n",
              "      <td>0.066806</td>\n",
              "      <td>0.210856</td>\n",
              "      <td>0.018789</td>\n",
              "      <td>0.576200</td>\n",
              "      <td>0.048017</td>\n",
              "      <td>0.486430</td>\n",
              "      <td>0.870564</td>\n",
              "      <td>0.486430</td>\n",
              "      <td>0.292276</td>\n",
              "      <td>0.100209</td>\n",
              "      <td>...</td>\n",
              "      <td>0.488518</td>\n",
              "      <td>0.087866</td>\n",
              "      <td>0.972860</td>\n",
              "      <td>0.350731</td>\n",
              "      <td>0.173640</td>\n",
              "      <td>0.918580</td>\n",
              "      <td>0.204593</td>\n",
              "      <td>0.839248</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.158664</td>\n",
              "      <td>0.413361</td>\n",
              "      <td>0.691023</td>\n",
              "      <td>0.348643</td>\n",
              "      <td>0.584551</td>\n",
              "      <td>0.762004</td>\n",
              "      <td>0.524008</td>\n",
              "      <td>0.895616</td>\n",
              "      <td>0.793319</td>\n",
              "      <td>0.453027</td>\n",
              "      <td>0.240084</td>\n",
              "      <td>0.354906</td>\n",
              "      <td>0.208768</td>\n",
              "      <td>0.688935</td>\n",
              "      <td>0.828810</td>\n",
              "      <td>0.254697</td>\n",
              "      <td>0.941423</td>\n",
              "      <td>0.724426</td>\n",
              "      <td>0.334029</td>\n",
              "      <td>0.701461</td>\n",
              "      <td>0.319415</td>\n",
              "      <td>0.680585</td>\n",
              "      <td>0.320084</td>\n",
              "      <td>0.300626</td>\n",
              "      <td>0.908142</td>\n",
              "      <td>0.179541</td>\n",
              "      <td>0.584551</td>\n",
              "      <td>0.356994</td>\n",
              "      <td>0.931106</td>\n",
              "      <td>0.083507</td>\n",
              "      <td>1937.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.420502</td>\n",
              "      <td>0.179541</td>\n",
              "      <td>0.772443</td>\n",
              "      <td>0.517745</td>\n",
              "      <td>0.261506</td>\n",
              "      <td>0.156576</td>\n",
              "      <td>0.561587</td>\n",
              "      <td>0.526096</td>\n",
              "      <td>0.528184</td>\n",
              "      <td>0.085774</td>\n",
              "      <td>0.638831</td>\n",
              "      <td>0.473904</td>\n",
              "      <td>0.509395</td>\n",
              "      <td>0.747390</td>\n",
              "      <td>0.487448</td>\n",
              "      <td>0.835073</td>\n",
              "      <td>0.108559</td>\n",
              "      <td>0.549061</td>\n",
              "      <td>0.640167</td>\n",
              "      <td>0.127349</td>\n",
              "      <td>0.475992</td>\n",
              "      <td>0.233820</td>\n",
              "      <td>0.588727</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>0.987474</td>\n",
              "      <td>0.461378</td>\n",
              "      <td>0.983299</td>\n",
              "      <td>0.701461</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.868476</td>\n",
              "      <td>0.561587</td>\n",
              "      <td>0.588727</td>\n",
              "      <td>0.645094</td>\n",
              "      <td>0.321503</td>\n",
              "      <td>0.843424</td>\n",
              "      <td>0.194154</td>\n",
              "      <td>0.187891</td>\n",
              "      <td>0.755741</td>\n",
              "      <td>0.361169</td>\n",
              "      <td>0.121086</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853862</td>\n",
              "      <td>0.922594</td>\n",
              "      <td>0.494781</td>\n",
              "      <td>0.903967</td>\n",
              "      <td>0.518828</td>\n",
              "      <td>0.137787</td>\n",
              "      <td>0.465553</td>\n",
              "      <td>0.093946</td>\n",
              "      <td>0.377871</td>\n",
              "      <td>0.613779</td>\n",
              "      <td>0.302714</td>\n",
              "      <td>0.559499</td>\n",
              "      <td>0.691023</td>\n",
              "      <td>0.127349</td>\n",
              "      <td>0.244259</td>\n",
              "      <td>0.780793</td>\n",
              "      <td>0.223382</td>\n",
              "      <td>0.141962</td>\n",
              "      <td>0.400835</td>\n",
              "      <td>0.457203</td>\n",
              "      <td>0.198330</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.855950</td>\n",
              "      <td>0.937370</td>\n",
              "      <td>0.467641</td>\n",
              "      <td>0.194561</td>\n",
              "      <td>0.628392</td>\n",
              "      <td>0.461378</td>\n",
              "      <td>0.956159</td>\n",
              "      <td>0.361169</td>\n",
              "      <td>0.620042</td>\n",
              "      <td>0.079498</td>\n",
              "      <td>0.561587</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.070981</td>\n",
              "      <td>0.196242</td>\n",
              "      <td>0.749478</td>\n",
              "      <td>0.413361</td>\n",
              "      <td>0.774530</td>\n",
              "      <td>1713.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.020921</td>\n",
              "      <td>0.480167</td>\n",
              "      <td>0.281837</td>\n",
              "      <td>0.772443</td>\n",
              "      <td>0.313808</td>\n",
              "      <td>0.908142</td>\n",
              "      <td>0.958246</td>\n",
              "      <td>0.283925</td>\n",
              "      <td>0.056367</td>\n",
              "      <td>0.648536</td>\n",
              "      <td>0.876827</td>\n",
              "      <td>0.455115</td>\n",
              "      <td>0.678497</td>\n",
              "      <td>0.503132</td>\n",
              "      <td>0.543933</td>\n",
              "      <td>0.052192</td>\n",
              "      <td>0.513570</td>\n",
              "      <td>0.626305</td>\n",
              "      <td>0.887029</td>\n",
              "      <td>0.027140</td>\n",
              "      <td>0.521921</td>\n",
              "      <td>0.179541</td>\n",
              "      <td>0.361169</td>\n",
              "      <td>0.506276</td>\n",
              "      <td>0.632568</td>\n",
              "      <td>0.043841</td>\n",
              "      <td>0.601253</td>\n",
              "      <td>0.519833</td>\n",
              "      <td>0.207113</td>\n",
              "      <td>0.056367</td>\n",
              "      <td>0.246347</td>\n",
              "      <td>0.793319</td>\n",
              "      <td>0.926931</td>\n",
              "      <td>0.440501</td>\n",
              "      <td>0.210856</td>\n",
              "      <td>0.594990</td>\n",
              "      <td>0.175365</td>\n",
              "      <td>0.561587</td>\n",
              "      <td>0.248434</td>\n",
              "      <td>0.050104</td>\n",
              "      <td>...</td>\n",
              "      <td>0.916493</td>\n",
              "      <td>0.721757</td>\n",
              "      <td>0.263048</td>\n",
              "      <td>0.563674</td>\n",
              "      <td>0.843096</td>\n",
              "      <td>0.321503</td>\n",
              "      <td>0.509395</td>\n",
              "      <td>0.542797</td>\n",
              "      <td>0.858038</td>\n",
              "      <td>0.964509</td>\n",
              "      <td>0.189979</td>\n",
              "      <td>0.534447</td>\n",
              "      <td>0.221294</td>\n",
              "      <td>0.901879</td>\n",
              "      <td>0.473904</td>\n",
              "      <td>0.279749</td>\n",
              "      <td>0.085595</td>\n",
              "      <td>0.895616</td>\n",
              "      <td>0.590814</td>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.628392</td>\n",
              "      <td>0.762004</td>\n",
              "      <td>0.246347</td>\n",
              "      <td>0.728601</td>\n",
              "      <td>0.292276</td>\n",
              "      <td>0.916318</td>\n",
              "      <td>0.617954</td>\n",
              "      <td>0.795407</td>\n",
              "      <td>0.221294</td>\n",
              "      <td>0.814196</td>\n",
              "      <td>0.682672</td>\n",
              "      <td>0.671548</td>\n",
              "      <td>0.127349</td>\n",
              "      <td>0.916493</td>\n",
              "      <td>0.747390</td>\n",
              "      <td>0.651357</td>\n",
              "      <td>0.200418</td>\n",
              "      <td>0.052192</td>\n",
              "      <td>0.052192</td>\n",
              "      <td>1721.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.815900</td>\n",
              "      <td>0.407098</td>\n",
              "      <td>0.133612</td>\n",
              "      <td>0.536534</td>\n",
              "      <td>0.746862</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.334029</td>\n",
              "      <td>0.446764</td>\n",
              "      <td>0.185804</td>\n",
              "      <td>0.315900</td>\n",
              "      <td>0.517745</td>\n",
              "      <td>0.599165</td>\n",
              "      <td>0.801670</td>\n",
              "      <td>0.686848</td>\n",
              "      <td>0.368201</td>\n",
              "      <td>0.050104</td>\n",
              "      <td>0.546973</td>\n",
              "      <td>0.227557</td>\n",
              "      <td>0.313808</td>\n",
              "      <td>0.365344</td>\n",
              "      <td>0.204593</td>\n",
              "      <td>0.125261</td>\n",
              "      <td>0.467641</td>\n",
              "      <td>0.194561</td>\n",
              "      <td>0.331942</td>\n",
              "      <td>0.991649</td>\n",
              "      <td>0.206681</td>\n",
              "      <td>0.093946</td>\n",
              "      <td>0.828452</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.574113</td>\n",
              "      <td>0.580376</td>\n",
              "      <td>0.897704</td>\n",
              "      <td>0.964509</td>\n",
              "      <td>0.079332</td>\n",
              "      <td>0.849687</td>\n",
              "      <td>0.137787</td>\n",
              "      <td>0.699374</td>\n",
              "      <td>0.334029</td>\n",
              "      <td>0.085595</td>\n",
              "      <td>...</td>\n",
              "      <td>0.342380</td>\n",
              "      <td>0.115063</td>\n",
              "      <td>0.615866</td>\n",
              "      <td>0.501044</td>\n",
              "      <td>0.759414</td>\n",
              "      <td>0.081420</td>\n",
              "      <td>0.960334</td>\n",
              "      <td>0.400835</td>\n",
              "      <td>0.106472</td>\n",
              "      <td>0.703549</td>\n",
              "      <td>0.845511</td>\n",
              "      <td>0.077244</td>\n",
              "      <td>0.056367</td>\n",
              "      <td>0.874739</td>\n",
              "      <td>0.755741</td>\n",
              "      <td>0.131524</td>\n",
              "      <td>0.572025</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>0.058455</td>\n",
              "      <td>0.697286</td>\n",
              "      <td>0.576200</td>\n",
              "      <td>0.482255</td>\n",
              "      <td>0.726514</td>\n",
              "      <td>0.920668</td>\n",
              "      <td>0.450939</td>\n",
              "      <td>0.485356</td>\n",
              "      <td>0.682672</td>\n",
              "      <td>0.244259</td>\n",
              "      <td>0.732777</td>\n",
              "      <td>0.010438</td>\n",
              "      <td>0.572025</td>\n",
              "      <td>0.075314</td>\n",
              "      <td>0.240084</td>\n",
              "      <td>0.498956</td>\n",
              "      <td>0.634656</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.887265</td>\n",
              "      <td>0.578288</td>\n",
              "      <td>0.559499</td>\n",
              "      <td>1706.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.146444</td>\n",
              "      <td>0.780793</td>\n",
              "      <td>0.997912</td>\n",
              "      <td>0.688935</td>\n",
              "      <td>0.449791</td>\n",
              "      <td>0.544885</td>\n",
              "      <td>0.215031</td>\n",
              "      <td>0.246347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.976987</td>\n",
              "      <td>0.409186</td>\n",
              "      <td>0.267223</td>\n",
              "      <td>0.906054</td>\n",
              "      <td>0.225470</td>\n",
              "      <td>0.640167</td>\n",
              "      <td>0.329854</td>\n",
              "      <td>0.572025</td>\n",
              "      <td>0.290188</td>\n",
              "      <td>0.102510</td>\n",
              "      <td>0.764092</td>\n",
              "      <td>0.858038</td>\n",
              "      <td>0.939457</td>\n",
              "      <td>0.787056</td>\n",
              "      <td>0.575314</td>\n",
              "      <td>0.974948</td>\n",
              "      <td>0.676409</td>\n",
              "      <td>0.835073</td>\n",
              "      <td>0.588727</td>\n",
              "      <td>0.393305</td>\n",
              "      <td>0.306889</td>\n",
              "      <td>0.979123</td>\n",
              "      <td>0.398747</td>\n",
              "      <td>0.716075</td>\n",
              "      <td>0.308977</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.881002</td>\n",
              "      <td>0.615866</td>\n",
              "      <td>0.365344</td>\n",
              "      <td>0.192067</td>\n",
              "      <td>0.747390</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200418</td>\n",
              "      <td>0.510460</td>\n",
              "      <td>0.423800</td>\n",
              "      <td>0.221294</td>\n",
              "      <td>0.958159</td>\n",
              "      <td>0.369520</td>\n",
              "      <td>0.636743</td>\n",
              "      <td>0.622129</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>0.164927</td>\n",
              "      <td>0.759916</td>\n",
              "      <td>0.866388</td>\n",
              "      <td>0.643006</td>\n",
              "      <td>0.835073</td>\n",
              "      <td>0.244259</td>\n",
              "      <td>0.317328</td>\n",
              "      <td>0.077244</td>\n",
              "      <td>0.613779</td>\n",
              "      <td>0.340292</td>\n",
              "      <td>0.167015</td>\n",
              "      <td>0.868476</td>\n",
              "      <td>0.814196</td>\n",
              "      <td>0.006263</td>\n",
              "      <td>0.751566</td>\n",
              "      <td>0.787056</td>\n",
              "      <td>0.493724</td>\n",
              "      <td>0.450939</td>\n",
              "      <td>0.169102</td>\n",
              "      <td>0.592902</td>\n",
              "      <td>0.283925</td>\n",
              "      <td>0.085595</td>\n",
              "      <td>0.780335</td>\n",
              "      <td>0.820459</td>\n",
              "      <td>0.229645</td>\n",
              "      <td>0.766180</td>\n",
              "      <td>0.643006</td>\n",
              "      <td>0.918580</td>\n",
              "      <td>0.252610</td>\n",
              "      <td>0.860125</td>\n",
              "      <td>1775.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2         3     ...      2497      2498      2499    2500\n",
              "0  0.284519  1.000000  0.551148  0.010438  ...  0.524008  0.588727  0.056367  1606.0\n",
              "1  0.133891  0.054280  0.526096  0.066806  ...  0.277662  0.989562  0.423800  1714.0\n",
              "2  0.100418  0.793319  0.288100  0.697286  ...  0.442589  0.336117  0.799582  1936.0\n",
              "3  0.794979  0.267223  0.008351  0.204593  ...  0.325678  0.793319  0.908142  1811.0\n",
              "4  0.016736  0.544885  0.294363  0.430063  ...  0.085595  0.739040  0.388309  1890.0\n",
              "5  0.822176  0.855950  0.899791  0.937370  ...  0.356994  0.931106  0.083507  1937.0\n",
              "6  0.420502  0.179541  0.772443  0.517745  ...  0.749478  0.413361  0.774530  1713.0\n",
              "7  0.020921  0.480167  0.281837  0.772443  ...  0.200418  0.052192  0.052192  1721.0\n",
              "8  0.815900  0.407098  0.133612  0.536534  ...  0.887265  0.578288  0.559499  1706.0\n",
              "9  0.146444  0.780793  0.997912  0.688935  ...  0.918580  0.252610  0.860125  1775.0\n",
              "\n",
              "[10 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2iijvf7csJ5",
        "colab_type": "text"
      },
      "source": [
        "Train & Test Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFoF652bdl-p",
        "colab": {}
      },
      "source": [
        "#Split Dataset in 80:20. \n",
        "#Random state is mentioned so that the partition remain same for each execution\n",
        "X = df.iloc[:, 0:len(df.columns)-1].values\n",
        "Y = df[2500].values\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.20, random_state=2003)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6eEfvGfsRH",
        "colab_type": "text"
      },
      "source": [
        "Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvASMuJ6fk1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the performance metrics (r2score and RMSE) using the Sklearn Lib\n",
        "def showResults(test, pred):\n",
        "  mse=mean_squared_error(test, pred)\n",
        "  rmse = sqrt(mse)\n",
        "  r2=r2_score(test, pred)\n",
        "  print(\"RMSE: \", rmse)\n",
        "  print(\"R2Score: \",r2)\n",
        "  result=[rmse,r2]\n",
        "  return result"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_kjSvAKd8Ss",
        "colab_type": "text"
      },
      "source": [
        "# ANN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J0lekaMczU2",
        "colab_type": "text"
      },
      "source": [
        "ANN Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcrDPFqiu6Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model\n",
        "verbose, epochs, batch_size = 1, 50, 32\n",
        "activationFunction='relu'\n",
        "\n",
        "def getANNModel():\n",
        "    annmodel = Sequential()\n",
        "    annmodel.add(Dense(32, input_dim = X.shape[1], activation=activationFunction))\n",
        "    annmodel.add(Dense(32, activation=activationFunction))\n",
        "    annmodel.add(Dense(1, activation='linear'))\n",
        "    annmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    return annmodel\n",
        "annmodel = getANNModel()"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgedoRJF_tiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "172c6f47-38b3-4137-8560-42ac79fdf94a"
      },
      "source": [
        "annmodel.summary()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_52 (Dense)             (None, 32)                80032     \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 81,121\n",
            "Trainable params: 81,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXAIvp_Mc5mR",
        "colab_type": "text"
      },
      "source": [
        "Train the ANN model using KFold Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt48NPLp2MR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b1db561-9f7a-415f-9dd6-3c2680a471c1"
      },
      "source": [
        "#Apply 7-Fold cross Validation to train the model\n",
        "annCv = KFold(n_splits=7, shuffle = True , random_state=10)\n",
        "annCv.get_n_splits(xTrain, yTrain)\n",
        "foldNum=0\n",
        "#Dividing the training data again in Training and Validation\n",
        "for train_index, val_index in annCv.split(xTrain, yTrain):\n",
        "    foldNum+=1\n",
        "    print(\"Results for fold\",foldNum)\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "    annhistory = annmodel.fit(X_train, Y_train, \n",
        "                        validation_data = (X_val, Y_val), \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size)  \n",
        "    yPredict = annmodel.predict(X_val)\n",
        "    showResults(Y_val, yPredict)\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for fold 1\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2921149.2500 - val_loss: 2838273.5000\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2629933.0000 - val_loss: 2428138.0000\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2113636.7500 - val_loss: 1784165.6250\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1395446.2500 - val_loss: 996745.3125\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 645641.4375 - val_loss: 330652.3125\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 154024.4375 - val_loss: 40820.8086\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 13809.9590 - val_loss: 7352.4858\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 7367.6226 - val_loss: 7442.7998\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6961.7490 - val_loss: 7448.6440\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6812.4795 - val_loss: 7508.3447\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6810.3848 - val_loss: 7449.9673\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6810.6758 - val_loss: 7476.3394\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6798.1680 - val_loss: 7445.3193\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6790.4766 - val_loss: 7479.5586\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6789.2520 - val_loss: 7484.4116\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6802.2139 - val_loss: 7417.1733\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6787.8140 - val_loss: 7503.3071\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6770.8477 - val_loss: 7435.2935\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6768.0005 - val_loss: 7474.2197\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6757.4487 - val_loss: 7479.4937\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6753.3906 - val_loss: 7499.9175\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6752.4849 - val_loss: 7482.1011\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6741.1772 - val_loss: 7491.8164\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6735.3477 - val_loss: 7475.5713\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6734.6890 - val_loss: 7415.7852\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6726.6611 - val_loss: 7504.1924\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6707.5801 - val_loss: 7431.5283\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6700.4385 - val_loss: 7498.6860\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6704.8999 - val_loss: 7456.8467\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6703.9883 - val_loss: 7517.1455\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6707.9409 - val_loss: 7483.5811\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6689.3257 - val_loss: 7505.4346\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6668.1655 - val_loss: 7456.5356\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6651.1899 - val_loss: 7431.9741\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6661.2847 - val_loss: 7482.3926\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6653.0503 - val_loss: 7481.4795\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6641.9502 - val_loss: 7447.1548\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6653.1841 - val_loss: 7456.9507\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6614.7798 - val_loss: 7433.5815\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6604.6992 - val_loss: 7509.6509\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6587.5459 - val_loss: 7428.2954\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6586.2534 - val_loss: 7415.2871\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6584.6812 - val_loss: 7449.6196\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 6561.9355 - val_loss: 7476.9985\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6562.5264 - val_loss: 7524.6206\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6567.4839 - val_loss: 7429.4609\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6537.0015 - val_loss: 7475.6099\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6522.4702 - val_loss: 7466.6221\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6515.9907 - val_loss: 7457.6074\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6499.3721 - val_loss: 7431.6045\n",
            "RMSE:  86.20675285975116\n",
            "R2Score:  0.02952251859686228\n",
            "Results for fold 2\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6855.9385 - val_loss: 5346.4888\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6816.8599 - val_loss: 5357.4888\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6829.6479 - val_loss: 5349.6421\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6798.4512 - val_loss: 5348.3672\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6783.1206 - val_loss: 5351.1157\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6793.7388 - val_loss: 5350.2070\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6775.1709 - val_loss: 5353.4492\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6815.3022 - val_loss: 5399.8018\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6750.1396 - val_loss: 5352.1187\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6711.9102 - val_loss: 5363.7056\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6694.6875 - val_loss: 5361.6865\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6690.8066 - val_loss: 5376.2314\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6685.6221 - val_loss: 5355.9580\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6654.6299 - val_loss: 5369.2593\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6637.4561 - val_loss: 5358.4658\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6654.6699 - val_loss: 5361.0234\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6629.8335 - val_loss: 5379.9902\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6594.8853 - val_loss: 5360.0811\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6589.0020 - val_loss: 5361.4727\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6581.1714 - val_loss: 5361.6240\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6545.4531 - val_loss: 5391.8638\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6545.5586 - val_loss: 5371.1069\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6538.2549 - val_loss: 5440.2324\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6503.4102 - val_loss: 5371.4790\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6539.5859 - val_loss: 5380.6675\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6489.7671 - val_loss: 5367.4810\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6467.2642 - val_loss: 5412.4619\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6452.4995 - val_loss: 5372.7520\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6433.9790 - val_loss: 5371.2119\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6426.3228 - val_loss: 5449.6792\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6383.8823 - val_loss: 5376.2646\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6361.0469 - val_loss: 5376.8506\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6349.0117 - val_loss: 5376.0103\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6374.7495 - val_loss: 5414.6255\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6330.3662 - val_loss: 5388.9741\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6337.7363 - val_loss: 5381.0103\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6303.0195 - val_loss: 5389.1211\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6277.2183 - val_loss: 5410.5811\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6239.5488 - val_loss: 5386.2212\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6244.2202 - val_loss: 5398.9824\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6213.0352 - val_loss: 5387.3311\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6194.0503 - val_loss: 5468.2886\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6164.2920 - val_loss: 5391.5615\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 6214.1841 - val_loss: 5414.9214\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6117.6509 - val_loss: 5400.3989\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6201.8765 - val_loss: 5618.3540\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6079.2188 - val_loss: 5408.3506\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6051.3140 - val_loss: 5400.7104\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6029.2900 - val_loss: 5396.9131\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 6014.3345 - val_loss: 5398.1255\n",
            "RMSE:  73.47193986288225\n",
            "R2Score:  0.17267597163787662\n",
            "Results for fold 3\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5914.5410 - val_loss: 5937.9766\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5873.3325 - val_loss: 5952.7129\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5849.2495 - val_loss: 5918.4014\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5841.8799 - val_loss: 5980.9492\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5838.4424 - val_loss: 5923.0420\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5776.8032 - val_loss: 6016.9702\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5781.2422 - val_loss: 6041.1870\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5800.6362 - val_loss: 5971.2144\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5735.1318 - val_loss: 5937.3164\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5740.2456 - val_loss: 6026.2314\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5677.1416 - val_loss: 5995.4614\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5740.1245 - val_loss: 5930.6792\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5650.1602 - val_loss: 5934.2827\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5656.6733 - val_loss: 6018.0991\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5571.1523 - val_loss: 5965.4429\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5614.4053 - val_loss: 5989.1475\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5605.7266 - val_loss: 5990.4937\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5598.9302 - val_loss: 6157.7026\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5520.6191 - val_loss: 5973.3838\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5484.5957 - val_loss: 5954.9370\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5576.0059 - val_loss: 5948.3545\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5480.2861 - val_loss: 6031.1982\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5462.7568 - val_loss: 5972.6782\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5419.8535 - val_loss: 5995.2598\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5365.1211 - val_loss: 5953.8350\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5326.3057 - val_loss: 5952.6279\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5334.7852 - val_loss: 5953.9302\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5266.0610 - val_loss: 5968.8901\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5252.0093 - val_loss: 6149.4814\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5248.4761 - val_loss: 5963.8809\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5190.7422 - val_loss: 6007.7334\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5219.5625 - val_loss: 6111.0747\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5193.6001 - val_loss: 6030.8145\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5206.5571 - val_loss: 6103.0518\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5124.7207 - val_loss: 5995.6382\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5080.2148 - val_loss: 6012.5854\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5103.3809 - val_loss: 5995.0293\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5008.8213 - val_loss: 6053.9058\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5030.9478 - val_loss: 6132.9927\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4962.0698 - val_loss: 6048.5830\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4943.3892 - val_loss: 6023.8213\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4883.3848 - val_loss: 6175.4761\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4895.6436 - val_loss: 6144.8442\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4874.6782 - val_loss: 6099.0674\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4862.4028 - val_loss: 6153.1748\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4788.2925 - val_loss: 6001.1914\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4789.2881 - val_loss: 6012.0039\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4760.6172 - val_loss: 6002.0527\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4725.9351 - val_loss: 6420.7856\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4721.3950 - val_loss: 6215.2178\n",
            "RMSE:  78.8366528512371\n",
            "R2Score:  0.22092480843944962\n",
            "Results for fold 4\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5106.1025 - val_loss: 4721.5552\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 5022.6011 - val_loss: 4369.3271\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4898.1890 - val_loss: 4408.1646\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4829.9658 - val_loss: 4389.6826\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4819.7075 - val_loss: 4422.3804\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4917.3960 - val_loss: 4383.6094\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4775.5962 - val_loss: 4381.8535\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4707.5366 - val_loss: 4370.8130\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4703.5332 - val_loss: 4379.7695\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4817.2734 - val_loss: 4405.0498\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4648.6738 - val_loss: 4373.2241\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4583.3960 - val_loss: 4456.7539\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4566.2798 - val_loss: 4377.7852\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4498.4434 - val_loss: 4404.1797\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4481.7271 - val_loss: 4421.7324\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4448.6660 - val_loss: 4392.2969\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4386.9199 - val_loss: 4482.3521\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4428.9346 - val_loss: 4437.6646\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4384.8130 - val_loss: 4387.2495\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4357.1626 - val_loss: 4408.4009\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4277.3071 - val_loss: 4405.4351\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 4288.7134 - val_loss: 4498.7935\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4269.6343 - val_loss: 4416.8584\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4181.8828 - val_loss: 4472.5830\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4158.9604 - val_loss: 4501.4580\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4190.6426 - val_loss: 4551.9312\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4069.5139 - val_loss: 4413.1807\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4170.9546 - val_loss: 4732.1152\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4056.3787 - val_loss: 4455.7871\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3955.3867 - val_loss: 4411.1147\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3966.9622 - val_loss: 4516.3887\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3943.3840 - val_loss: 4413.7173\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3904.0554 - val_loss: 4629.3872\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3887.9473 - val_loss: 4518.2979\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3889.8999 - val_loss: 4513.3799\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3794.0312 - val_loss: 4415.9658\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3775.9246 - val_loss: 4493.8633\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3719.1257 - val_loss: 4456.4067\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3675.1504 - val_loss: 4493.3408\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3686.9563 - val_loss: 4451.3325\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3572.7271 - val_loss: 4440.5234\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3575.1304 - val_loss: 4468.2705\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3486.1433 - val_loss: 4478.3823\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3453.7610 - val_loss: 4439.2148\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3438.4155 - val_loss: 4433.9019\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3418.2188 - val_loss: 4589.4985\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3369.6499 - val_loss: 4531.9897\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3308.8337 - val_loss: 4446.0527\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3269.5737 - val_loss: 4443.7021\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3258.2551 - val_loss: 4451.8984\n",
            "RMSE:  66.72254869358578\n",
            "R2Score:  0.41742530033956726\n",
            "Results for fold 5\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3482.6074 - val_loss: 2737.4048\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3455.0894 - val_loss: 2797.9448\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3398.7188 - val_loss: 2748.3149\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3358.6184 - val_loss: 2780.2478\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3293.7729 - val_loss: 3199.6467\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3330.0164 - val_loss: 2980.0647\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3273.4062 - val_loss: 3466.3286\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3308.0007 - val_loss: 2851.5171\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3171.9956 - val_loss: 2755.2949\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3324.2217 - val_loss: 2750.6824\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3173.9299 - val_loss: 2842.2764\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3129.2461 - val_loss: 3321.1758\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 3044.7949 - val_loss: 2920.3728\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2989.5964 - val_loss: 2804.6633\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2909.1877 - val_loss: 2758.9377\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2904.4385 - val_loss: 2784.3132\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2861.9707 - val_loss: 2796.6245\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2815.3584 - val_loss: 2824.8887\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2836.0273 - val_loss: 2779.5149\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2717.3977 - val_loss: 2855.1436\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2676.5649 - val_loss: 2930.4612\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2847.1392 - val_loss: 2903.0059\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2690.3044 - val_loss: 2775.5930\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2559.5649 - val_loss: 2790.0845\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2525.8613 - val_loss: 2817.8845\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2578.8596 - val_loss: 2784.6985\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2519.6602 - val_loss: 2823.0449\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2418.8533 - val_loss: 2819.5452\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2356.7537 - val_loss: 2836.0786\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2366.7742 - val_loss: 2883.1814\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2438.2913 - val_loss: 2814.1199\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2248.5505 - val_loss: 2897.1541\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2240.4429 - val_loss: 2863.0159\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 2200.4106 - val_loss: 2964.3132\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2400.1951 - val_loss: 2876.2866\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2196.4519 - val_loss: 2992.1816\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2066.3496 - val_loss: 2863.6521\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2037.6868 - val_loss: 2845.9670\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2041.7615 - val_loss: 2921.2429\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2031.5825 - val_loss: 2865.4146\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1964.0767 - val_loss: 2829.2031\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1871.3350 - val_loss: 3069.7073\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1862.3932 - val_loss: 2946.9128\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1852.5099 - val_loss: 2963.1658\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1781.2947 - val_loss: 2867.2180\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1815.3252 - val_loss: 3225.7344\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1780.4464 - val_loss: 2872.9937\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1714.8612 - val_loss: 2966.9585\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1754.3853 - val_loss: 3309.3706\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1778.4897 - val_loss: 2876.9421\n",
            "RMSE:  53.6371364126865\n",
            "R2Score:  0.6119948304270595\n",
            "Results for fold 6\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1900.0613 - val_loss: 1112.0096\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1900.7471 - val_loss: 1119.6826\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1829.6923 - val_loss: 1120.6058\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1815.0066 - val_loss: 1118.7732\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1808.4623 - val_loss: 1195.2788\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1745.0066 - val_loss: 1111.4399\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1677.3063 - val_loss: 1172.9617\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1625.3324 - val_loss: 1140.3352\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1691.9918 - val_loss: 1117.6243\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1576.4568 - val_loss: 1114.1222\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1538.0433 - val_loss: 1163.1362\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1558.5138 - val_loss: 1346.0818\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1555.3291 - val_loss: 1184.3862\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1458.1423 - val_loss: 1136.4701\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1411.0127 - val_loss: 1153.2476\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1406.2074 - val_loss: 1172.3771\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1338.7266 - val_loss: 1195.1631\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1331.0929 - val_loss: 1157.4366\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1340.0012 - val_loss: 1206.3015\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1262.9950 - val_loss: 1210.0297\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1236.7642 - val_loss: 1297.5897\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1229.3877 - val_loss: 1191.7356\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1194.3335 - val_loss: 1225.9911\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1170.6240 - val_loss: 1233.0393\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1148.8289 - val_loss: 1216.9351\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1082.5044 - val_loss: 1214.4061\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1059.0447 - val_loss: 1291.3815\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1062.6194 - val_loss: 1248.6477\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1050.3807 - val_loss: 1241.7280\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 1033.1818 - val_loss: 1246.0154\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 979.8084 - val_loss: 1274.1349\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 993.4261 - val_loss: 1336.3563\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 947.4800 - val_loss: 1280.7961\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 903.4649 - val_loss: 1298.1998\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 876.4993 - val_loss: 1325.8132\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 882.9982 - val_loss: 1306.2637\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 833.1136 - val_loss: 1343.7756\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 828.9125 - val_loss: 1331.9739\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 788.9106 - val_loss: 1336.9165\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 759.2952 - val_loss: 1352.9073\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 756.7271 - val_loss: 1351.1086\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 765.5434 - val_loss: 1358.4077\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 718.7859 - val_loss: 1368.9940\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 716.1345 - val_loss: 1414.8317\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 665.2596 - val_loss: 1421.6932\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 681.4741 - val_loss: 1404.6273\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 653.4734 - val_loss: 1411.6373\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 619.6456 - val_loss: 1427.0027\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 629.5033 - val_loss: 1439.3585\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 599.8297 - val_loss: 1545.7029\n",
            "RMSE:  39.3154296703163\n",
            "R2Score:  0.765286209131985\n",
            "Results for fold 7\n",
            "Epoch 1/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 786.7620 - val_loss: 468.2001\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 729.8891 - val_loss: 513.8832\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 756.6656 - val_loss: 599.2432\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 718.4464 - val_loss: 486.8901\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 688.5865 - val_loss: 611.2959\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 761.3681 - val_loss: 772.1290\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 722.1333 - val_loss: 506.3304\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 652.1492 - val_loss: 513.1711\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 610.5654 - val_loss: 563.6297\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 585.1840 - val_loss: 534.1262\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 556.4660 - val_loss: 537.4833\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 550.8420 - val_loss: 540.4382\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 561.6095 - val_loss: 558.3105\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 548.6658 - val_loss: 552.6181\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 515.2884 - val_loss: 614.9867\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 528.2735 - val_loss: 567.6456\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 521.5818 - val_loss: 622.0110\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 484.1531 - val_loss: 590.9871\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 443.9349 - val_loss: 699.2048\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 481.0096 - val_loss: 623.0960\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 432.5507 - val_loss: 601.4202\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 420.1218 - val_loss: 604.9855\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 399.0608 - val_loss: 622.7685\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 405.7328 - val_loss: 619.9226\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 375.7455 - val_loss: 689.4367\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 387.4670 - val_loss: 634.0470\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 375.4887 - val_loss: 640.0010\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 349.5991 - val_loss: 646.2478\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 351.4031 - val_loss: 654.6918\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 335.7900 - val_loss: 668.0145\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 334.3527 - val_loss: 752.2771\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 329.7857 - val_loss: 680.3832\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 309.3797 - val_loss: 683.0230\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 308.5267 - val_loss: 709.2016\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 284.9217 - val_loss: 713.2651\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 287.0204 - val_loss: 743.0917\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 271.6795 - val_loss: 723.6857\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 263.8150 - val_loss: 722.3615\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 246.0635 - val_loss: 726.6562\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 243.5288 - val_loss: 731.2731\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 238.7272 - val_loss: 805.3085\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 252.9939 - val_loss: 745.1534\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 229.9600 - val_loss: 774.8043\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 249.2999 - val_loss: 804.6659\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 236.0098 - val_loss: 784.5104\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 203.2533 - val_loss: 774.9574\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 197.7664 - val_loss: 780.0005\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 194.2820 - val_loss: 797.0479\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 190.7404 - val_loss: 800.1741\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 179.2388 - val_loss: 803.3424\n",
            "RMSE:  28.343296337348747\n",
            "R2Score:  0.9100701766967061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs44Ub-TdeYj",
        "colab_type": "text"
      },
      "source": [
        "Save and Load the ANN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjQnjin9ErAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annmodel.save(\"/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/0892691-ANN.h5\")"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aszCGrPoEy-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annmodel=load_model(\"/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/0892691-ANN.h5\")"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDqjpm9dhgk",
        "colab_type": "text"
      },
      "source": [
        "Test the ANN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qsh9TPS7dRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1933209e-405a-46da-ae80-291e8f3beb2b"
      },
      "source": [
        "#Test the model and calculate the performance metrics\n",
        "ann_yPred = annmodel.predict(xTest)\n",
        "ann_perf_metrics=showResults(yTest, ann_yPred)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE:  39.49130901336943\n",
            "R2Score:  0.7501263516757799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cciX6LdeeBzU",
        "colab_type": "text"
      },
      "source": [
        "Plot Actual value vs Predicted value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tabuJI3hzj5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "dab62c78-903a-4116-a41a-d98763a64385"
      },
      "source": [
        "#In ideal case if the actual value matches the predicted value \n",
        "#we would get a diagonal on the graph on mapping (actual,predicted) point\n",
        "#thus in the below graph we plot the diagonal \n",
        "#and then map the (actual,predicted) point to see the deviation\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(yTest, ann_yPred,color='green')\n",
        "ax.plot([yTest.min(), yTest.max()], [yTest.min(), yTest.max()],'k--',color='blue',lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "#plt.show()\n",
        "plt.savefig('/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/ANN.png', format='png', dpi=1200)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2pP5JVdeQUh",
        "colab_type": "text"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHWKtFLlecjB",
        "colab_type": "text"
      },
      "source": [
        "Reshape the data to 3D for Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vMG-gagRti7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "93484643-f04c-4642-f70e-1ff158e07e96"
      },
      "source": [
        "xTrain=xTrain.reshape(xTrain.shape[0],xTrain.shape[1],1)\n",
        "xTest=xTest.reshape(xTest.shape[0],xTest.shape[1],1)\n",
        "print(xTest.shape)\n",
        "print(xTrain.shape)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 2500, 1)\n",
            "(800, 2500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7aX8kyLeohb",
        "colab_type": "text"
      },
      "source": [
        "CNN Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjgi2OzaH9Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the Conv1d Model\n",
        "verbose, epochs, batch_size = 1, 100, 32\n",
        "activationFunction='relu'\n",
        "\n",
        "def getCNNModel():\n",
        "    cnnmodel = Sequential()\n",
        "    cnnmodel.add(Conv1D(32, 2, activation=\"relu\", input_shape=(xTrain.shape[1],xTrain.shape[2])))\n",
        "    cnnmodel.add(MaxPooling1D(pool_size=2))\n",
        "    cnnmodel.add(Dropout(rate=0.2))\n",
        "    cnnmodel.add(Flatten())\n",
        "    cnnmodel.add(Dense(64, activation=\"relu\"))\n",
        "    cnnmodel.add(Dense(1))\n",
        "    cnnmodel.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "    return cnnmodel\n",
        "\n",
        "cnnmodel = getCNNModel()"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BNP6kjtR5YR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1d15cac7-02f4-4573-ba92-88dc72a92180"
      },
      "source": [
        "cnnmodel.summary()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_15 (Conv1D)           (None, 2499, 32)          96        \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 1249, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 1249, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 39968)             0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 64)                2558016   \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,558,177\n",
            "Trainable params: 2,558,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtuHVv23ewnV",
        "colab_type": "text"
      },
      "source": [
        "Train the CNN model using KFold Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAfTsFOPIXu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "376cc57d-d07b-42ac-b21a-cd9275a3fdea"
      },
      "source": [
        "#Apply K-Fold cross Validation to train the model\n",
        "cnnCv = KFold(n_splits=5, shuffle = True , random_state=10)\n",
        "cnnCv.get_n_splits(xTrain, yTrain)\n",
        "foldNum=0\n",
        "for train_index, val_index in cnnCv.split(xTrain, yTrain):\n",
        "    foldNum+=1\n",
        "    print(\"Results for fold\",foldNum)\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "\n",
        "    X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
        "    X_val=X_val.reshape(X_val.shape[0],X_val.shape[1],1)\n",
        "    \n",
        "    cnnhistory = cnnmodel.fit(X_train, Y_train, \n",
        "                        validation_data = (X_val, Y_val), \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size)  \n",
        "    yPredict = cnnmodel.predict(X_val)\n",
        "    showResults(Y_val, yPredict)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for fold 1\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 2398827.5000 - val_loss: 1624448.7500\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 789719.9375 - val_loss: 100608.5312\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 38047.7109 - val_loss: 34491.3906\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 14739.4639 - val_loss: 12498.3008\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 8724.8896 - val_loss: 6924.0479\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7582.9688 - val_loss: 7120.5171\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7425.8076 - val_loss: 6945.9922\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7400.6655 - val_loss: 6985.6045\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7376.7041 - val_loss: 7001.0664\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7418.4243 - val_loss: 7010.9141\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7424.4658 - val_loss: 6910.2744\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7444.3750 - val_loss: 6950.1514\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7311.9141 - val_loss: 7062.3071\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7383.3257 - val_loss: 6894.0186\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7455.5391 - val_loss: 6916.1533\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7255.6602 - val_loss: 7019.5752\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7381.8813 - val_loss: 7017.6436\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7360.2212 - val_loss: 6988.7734\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 7312.9834 - val_loss: 6899.1382\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7378.0376 - val_loss: 7167.5859\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7318.4414 - val_loss: 6909.1055\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7311.5635 - val_loss: 6990.4053\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 7323.3525 - val_loss: 7089.7295\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7233.0181 - val_loss: 6872.4404\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7324.6812 - val_loss: 7189.3008\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7248.4580 - val_loss: 6920.5498\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7242.2905 - val_loss: 7139.5117\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7303.7397 - val_loss: 6873.7969\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7358.7041 - val_loss: 6963.3867\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7326.2095 - val_loss: 6882.5859\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7362.4624 - val_loss: 6941.9438\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7132.3994 - val_loss: 6887.1201\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 7259.4482 - val_loss: 7405.7749\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7062.8804 - val_loss: 6924.5376\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7298.7031 - val_loss: 6962.0029\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7170.2056 - val_loss: 7266.3047\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7151.8110 - val_loss: 6926.1079\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7098.7515 - val_loss: 6882.4736\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7070.2227 - val_loss: 7178.0493\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7159.5562 - val_loss: 6882.5508\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7163.7788 - val_loss: 6911.3687\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7021.9248 - val_loss: 7231.2256\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7071.6694 - val_loss: 6883.4619\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7168.2158 - val_loss: 6896.7876\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7036.0947 - val_loss: 7020.6680\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7067.3657 - val_loss: 6970.6274\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7027.8491 - val_loss: 6934.7969\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7029.6929 - val_loss: 7185.3555\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6982.9141 - val_loss: 6931.4912\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7030.8140 - val_loss: 7798.5947\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6931.6064 - val_loss: 6903.4727\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6944.2173 - val_loss: 6881.5142\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6872.0850 - val_loss: 6884.4189\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6928.1289 - val_loss: 6944.2373\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6804.5986 - val_loss: 7020.3086\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 7043.6553 - val_loss: 6878.9883\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6794.9438 - val_loss: 6877.9424\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6832.1455 - val_loss: 6896.7812\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6732.9561 - val_loss: 6930.6094\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6810.7710 - val_loss: 6886.1274\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6783.2236 - val_loss: 6947.6421\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6771.6665 - val_loss: 6883.4126\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6754.4390 - val_loss: 7049.2725\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6841.7100 - val_loss: 6947.4438\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6705.1875 - val_loss: 6883.3267\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6631.3594 - val_loss: 6967.7783\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6712.4897 - val_loss: 7128.0264\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6529.8730 - val_loss: 6885.5337\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6750.7983 - val_loss: 6915.0264\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6648.8828 - val_loss: 7300.1885\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6489.0884 - val_loss: 6992.6313\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6417.1143 - val_loss: 7341.9717\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6595.1094 - val_loss: 7839.2656\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6415.8208 - val_loss: 6948.5420\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6726.9834 - val_loss: 6956.9390\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6441.1201 - val_loss: 7227.3828\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6576.4272 - val_loss: 8039.9517\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6396.7393 - val_loss: 7242.8545\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6284.4795 - val_loss: 6892.0210\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6375.5186 - val_loss: 7423.2549\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6282.1147 - val_loss: 7158.7783\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6211.2031 - val_loss: 7054.5728\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6143.2104 - val_loss: 6897.2266\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6190.1768 - val_loss: 6900.1860\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6159.1182 - val_loss: 6928.3047\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6081.8516 - val_loss: 6897.9170\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6141.0015 - val_loss: 6922.6494\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6075.9951 - val_loss: 6900.2124\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6007.7505 - val_loss: 7224.0991\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6031.7256 - val_loss: 7004.8369\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6060.0562 - val_loss: 6997.4312\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5886.3936 - val_loss: 7046.4727\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5867.9351 - val_loss: 7345.1807\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5849.9307 - val_loss: 6925.9482\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5847.7886 - val_loss: 7550.4092\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5832.2944 - val_loss: 7420.8804\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5764.8765 - val_loss: 7011.7876\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5749.7319 - val_loss: 6988.1157\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5556.2334 - val_loss: 6941.4155\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 5718.4390 - val_loss: 6923.4453\n",
            "RMSE:  83.2072442207262\n",
            "R2Score:  0.017543678751599567\n",
            "Results for fold 2\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6286.5757 - val_loss: 5949.9268\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6284.0127 - val_loss: 5247.3760\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5994.6260 - val_loss: 5362.8091\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 6027.7368 - val_loss: 5270.3149\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5896.1118 - val_loss: 5382.3560\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5908.2173 - val_loss: 5204.8257\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 5796.8936 - val_loss: 5261.5146\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5910.3335 - val_loss: 5898.5908\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5848.0986 - val_loss: 5178.5703\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5728.4736 - val_loss: 5230.0991\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5594.5742 - val_loss: 5260.3525\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5610.1025 - val_loss: 5170.2471\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5550.9658 - val_loss: 5201.7095\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5552.0029 - val_loss: 5238.8110\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5470.7339 - val_loss: 5168.3711\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5544.6431 - val_loss: 5647.7925\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5566.4551 - val_loss: 5113.4404\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5312.6855 - val_loss: 5260.3442\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5317.4971 - val_loss: 5189.5342\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5334.7432 - val_loss: 5091.1587\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5171.1831 - val_loss: 5091.8813\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5087.2607 - val_loss: 5151.8032\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5205.8135 - val_loss: 5054.7983\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5289.5444 - val_loss: 5334.3882\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5040.6235 - val_loss: 5592.9839\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5286.1733 - val_loss: 5114.2915\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5259.8076 - val_loss: 5016.7788\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 5137.2041 - val_loss: 5060.1460\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4732.9976 - val_loss: 5188.0391\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4954.2188 - val_loss: 5592.5229\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4757.5132 - val_loss: 4981.9419\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4734.4961 - val_loss: 5256.6733\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4718.3096 - val_loss: 5088.2686\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4619.1289 - val_loss: 4944.8267\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4483.1299 - val_loss: 4957.8408\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4397.2051 - val_loss: 5040.8262\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4953.3735 - val_loss: 5446.7471\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4584.1260 - val_loss: 4913.0586\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4259.6553 - val_loss: 5079.7510\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4167.2881 - val_loss: 4871.6191\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4145.9741 - val_loss: 4864.9058\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4083.2019 - val_loss: 4861.3350\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 4001.3149 - val_loss: 4850.6030\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3861.8052 - val_loss: 4833.6226\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3889.7957 - val_loss: 4951.4971\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 3975.2214 - val_loss: 4922.1938\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3789.0952 - val_loss: 4793.9443\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3675.8677 - val_loss: 4793.7280\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3554.3860 - val_loss: 4796.3545\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3483.6372 - val_loss: 4756.7998\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3546.1606 - val_loss: 4743.3584\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3512.2097 - val_loss: 4765.1157\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3435.1841 - val_loss: 4736.9004\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3435.5071 - val_loss: 4865.3643\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3286.8071 - val_loss: 4700.4580\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3202.1094 - val_loss: 4779.5649\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3100.3694 - val_loss: 4728.2993\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3111.2378 - val_loss: 4682.2812\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3039.8459 - val_loss: 4744.8525\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2951.8923 - val_loss: 4812.0615\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2961.5359 - val_loss: 4714.5518\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3191.5237 - val_loss: 6160.5352\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 3249.3853 - val_loss: 4630.6108\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2759.6870 - val_loss: 4661.2881\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2700.7339 - val_loss: 4639.3428\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2738.3147 - val_loss: 5160.7319\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2557.4829 - val_loss: 4677.9102\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2616.9998 - val_loss: 4836.6729\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2448.7847 - val_loss: 4817.3486\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2467.1240 - val_loss: 4842.6201\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2589.0337 - val_loss: 4583.6602\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2306.9341 - val_loss: 4569.4453\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2326.5269 - val_loss: 4581.0200\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2301.2817 - val_loss: 4583.6641\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2249.2634 - val_loss: 4559.2368\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2156.7837 - val_loss: 4561.1045\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2074.0564 - val_loss: 4553.9312\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2050.2612 - val_loss: 4751.2017\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2251.6897 - val_loss: 5123.9912\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2175.8335 - val_loss: 4616.9033\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2051.0046 - val_loss: 4822.7607\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1978.9133 - val_loss: 4720.5771\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1861.7926 - val_loss: 4542.2046\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1833.8265 - val_loss: 4642.8467\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1757.0779 - val_loss: 4577.3135\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1728.8551 - val_loss: 4775.5249\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1942.8376 - val_loss: 5042.3643\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1813.4172 - val_loss: 4739.3530\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1621.0107 - val_loss: 4624.4995\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1697.5332 - val_loss: 4563.1001\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1453.5453 - val_loss: 4555.9351\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1669.3971 - val_loss: 4792.6562\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1703.6116 - val_loss: 4673.5674\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1470.9415 - val_loss: 4701.7007\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1430.1337 - val_loss: 4616.2529\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1436.0736 - val_loss: 4565.7617\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1451.1312 - val_loss: 4654.3008\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1421.3186 - val_loss: 4570.1235\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1366.9983 - val_loss: 4593.5059\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1503.3693 - val_loss: 4626.8657\n",
            "RMSE:  68.02106734370139\n",
            "R2Score:  0.3587276982594446\n",
            "Results for fold 3\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2267.7620 - val_loss: 1627.9246\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2410.8176 - val_loss: 830.3771\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2156.7341 - val_loss: 1010.2882\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2364.2837 - val_loss: 1104.4817\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2347.6782 - val_loss: 853.5160\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2097.0806 - val_loss: 925.3177\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2098.8716 - val_loss: 1129.0652\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1997.1008 - val_loss: 955.6437\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2025.9456 - val_loss: 838.4266\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1958.5117 - val_loss: 879.9323\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2134.2114 - val_loss: 855.2539\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 2044.0156 - val_loss: 939.0687\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1835.0117 - val_loss: 910.9922\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1773.2334 - val_loss: 886.6654\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1665.5140 - val_loss: 886.2889\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1688.7373 - val_loss: 862.2074\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1627.5560 - val_loss: 1001.1945\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1820.6414 - val_loss: 865.1240\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1628.7194 - val_loss: 891.4293\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1568.9325 - val_loss: 870.9715\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1520.9982 - val_loss: 927.8888\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1405.1759 - val_loss: 905.2160\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1501.3749 - val_loss: 924.4440\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1484.9763 - val_loss: 1235.0580\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1359.6975 - val_loss: 943.3123\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1377.2485 - val_loss: 945.5367\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1319.7168 - val_loss: 953.4915\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1646.8531 - val_loss: 975.2306\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1293.7524 - val_loss: 955.1669\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1254.2939 - val_loss: 917.4164\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1324.0498 - val_loss: 927.5242\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1263.1471 - val_loss: 1009.2992\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1185.6368 - val_loss: 945.3030\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1209.8108 - val_loss: 1076.3162\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1234.0879 - val_loss: 984.1291\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1120.1323 - val_loss: 962.5099\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1100.4958 - val_loss: 954.7405\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1127.3308 - val_loss: 967.3338\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1150.5958 - val_loss: 967.1887\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1012.3527 - val_loss: 1011.3623\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1097.2344 - val_loss: 980.5062\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1047.1294 - val_loss: 1006.3149\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1240.0455 - val_loss: 1015.8660\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1114.2266 - val_loss: 1012.3236\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 993.7932 - val_loss: 1162.9485\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1037.8582 - val_loss: 1553.5626\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1129.7578 - val_loss: 1587.1685\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1035.8875 - val_loss: 1190.5603\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1010.2684 - val_loss: 1102.7655\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 978.5681 - val_loss: 1096.2244\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 951.2319 - val_loss: 1055.2653\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 894.3474 - val_loss: 1319.3469\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 885.7939 - val_loss: 1111.7723\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1007.8973 - val_loss: 1668.3323\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 983.8660 - val_loss: 1121.3313\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 835.6076 - val_loss: 1127.6295\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 936.1937 - val_loss: 1286.6443\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 887.8340 - val_loss: 1105.3873\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 775.5239 - val_loss: 1141.2609\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 840.2959 - val_loss: 1139.0070\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 771.7159 - val_loss: 1245.9709\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 855.0212 - val_loss: 1162.8711\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 794.3827 - val_loss: 1230.7402\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 777.4366 - val_loss: 1175.6204\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 748.9633 - val_loss: 1158.4778\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 756.5990 - val_loss: 1176.1198\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 670.3488 - val_loss: 1164.8619\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 711.2735 - val_loss: 1203.9412\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 734.1522 - val_loss: 1489.8901\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 830.3259 - val_loss: 1187.3138\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 683.4138 - val_loss: 1270.7076\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 691.3009 - val_loss: 1218.0331\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 670.6158 - val_loss: 1420.7528\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 638.9842 - val_loss: 1301.0337\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 682.2639 - val_loss: 1312.4130\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 921.1721 - val_loss: 1555.3562\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 684.0388 - val_loss: 1240.7020\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 667.3102 - val_loss: 1287.2736\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 633.6953 - val_loss: 1259.6322\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 655.3616 - val_loss: 1304.0701\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 689.8893 - val_loss: 1271.9502\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 601.7151 - val_loss: 1260.9729\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 577.0918 - val_loss: 1502.5287\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 780.2164 - val_loss: 1302.6560\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 800.4004 - val_loss: 1282.9841\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 551.6973 - val_loss: 1290.1216\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 550.4427 - val_loss: 1320.0929\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 621.6749 - val_loss: 1305.2101\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 597.7482 - val_loss: 1558.2336\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 606.3751 - val_loss: 1339.6575\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 543.7001 - val_loss: 1351.0928\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 606.5247 - val_loss: 1330.1238\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 550.2283 - val_loss: 1339.6591\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 553.7092 - val_loss: 1342.9718\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 584.4716 - val_loss: 1542.0480\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 699.9343 - val_loss: 1359.1692\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 545.6476 - val_loss: 1382.3738\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 729.3827 - val_loss: 1407.5699\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 607.9482 - val_loss: 1503.8324\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 608.2915 - val_loss: 1385.5715\n",
            "RMSE:  37.223265791113995\n",
            "R2Score:  0.8269800264914795\n",
            "Results for fold 4\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 838.4559 - val_loss: 276.2107\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 974.7419 - val_loss: 116.9843\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1213.9261 - val_loss: 203.1037\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 973.6403 - val_loss: 882.3022\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1061.8196 - val_loss: 222.5755\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 840.7314 - val_loss: 124.8627\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 812.0066 - val_loss: 125.6720\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 811.3668 - val_loss: 127.5000\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 870.2409 - val_loss: 131.4016\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 731.7719 - val_loss: 203.9356\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 761.7740 - val_loss: 136.3507\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 738.9957 - val_loss: 139.1980\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 681.9841 - val_loss: 173.5123\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 648.2379 - val_loss: 222.8773\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 638.7317 - val_loss: 214.9225\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 680.3206 - val_loss: 154.7437\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 662.8588 - val_loss: 226.6400\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 717.5406 - val_loss: 210.5146\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 662.9242 - val_loss: 176.8396\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 774.1917 - val_loss: 441.5596\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 870.5822 - val_loss: 830.1285\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 774.9467 - val_loss: 184.7345\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 626.9530 - val_loss: 192.6895\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 562.8251 - val_loss: 198.1366\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 631.3115 - val_loss: 195.8985\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 611.1038 - val_loss: 299.6349\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 565.4266 - val_loss: 201.1023\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 583.2155 - val_loss: 253.3512\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 574.9013 - val_loss: 352.3629\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 779.7926 - val_loss: 378.5006\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 566.8610 - val_loss: 252.2054\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 604.0243 - val_loss: 451.4116\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 516.5787 - val_loss: 232.5173\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 687.6625 - val_loss: 372.0609\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 707.8027 - val_loss: 235.2265\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 600.1650 - val_loss: 247.7736\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 532.8582 - val_loss: 260.7914\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 610.6508 - val_loss: 269.2035\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 562.4161 - val_loss: 272.3592\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 534.5295 - val_loss: 270.2014\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 524.8107 - val_loss: 288.0634\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 521.6656 - val_loss: 280.1893\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 522.2439 - val_loss: 360.6937\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 471.2294 - val_loss: 279.4183\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 675.2776 - val_loss: 339.9397\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 654.6325 - val_loss: 323.6883\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 527.7525 - val_loss: 294.9622\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 525.0173 - val_loss: 300.0912\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 563.4371 - val_loss: 619.7015\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 641.9149 - val_loss: 316.4329\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 458.0155 - val_loss: 410.0219\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 498.5102 - val_loss: 413.6155\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 522.9490 - val_loss: 348.3283\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 528.0568 - val_loss: 475.5237\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 486.6916 - val_loss: 422.1445\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 493.2762 - val_loss: 463.7377\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 553.7904 - val_loss: 590.3810\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 546.9381 - val_loss: 448.4776\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 511.6049 - val_loss: 356.0156\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 537.0111 - val_loss: 365.4552\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 452.6121 - val_loss: 365.6351\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 457.9908 - val_loss: 370.3273\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 590.6631 - val_loss: 475.5994\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 449.7559 - val_loss: 391.4312\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 448.9689 - val_loss: 387.3875\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 476.6391 - val_loss: 563.2285\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 483.4301 - val_loss: 394.3875\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 426.2698 - val_loss: 396.9330\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 452.0222 - val_loss: 448.4727\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 455.9371 - val_loss: 485.6456\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 532.3055 - val_loss: 414.5899\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 410.1498 - val_loss: 714.3685\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 599.9554 - val_loss: 424.8075\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 470.6147 - val_loss: 471.7342\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 405.3973 - val_loss: 503.7629\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 557.3335 - val_loss: 733.0400\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 632.0342 - val_loss: 499.4332\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 438.4378 - val_loss: 451.1654\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 453.4261 - val_loss: 542.5524\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 456.0382 - val_loss: 522.3635\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 450.9294 - val_loss: 479.4971\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 513.4376 - val_loss: 891.5309\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 560.0895 - val_loss: 478.9190\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 444.4298 - val_loss: 548.2087\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 471.7012 - val_loss: 620.4333\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 455.3633 - val_loss: 498.4203\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 489.2711 - val_loss: 551.3454\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 447.0012 - val_loss: 543.7817\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 417.5678 - val_loss: 497.5941\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 427.4813 - val_loss: 511.7813\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 421.5026 - val_loss: 680.0223\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 429.2308 - val_loss: 515.3629\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 454.5324 - val_loss: 578.2778\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 408.4083 - val_loss: 528.7402\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 445.6174 - val_loss: 767.9066\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 510.8634 - val_loss: 603.3680\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 485.7738 - val_loss: 537.7855\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 390.7115 - val_loss: 544.1390\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 464.1927 - val_loss: 562.9505\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 393.2466 - val_loss: 551.6289\n",
            "RMSE:  23.48678160026011\n",
            "R2Score:  0.9236990520455652\n",
            "Results for fold 5\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 554.8285 - val_loss: 52.8793\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 539.7606 - val_loss: 396.6694\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 705.3307 - val_loss: 159.6079\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 634.5646 - val_loss: 81.3182\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 544.3802 - val_loss: 187.8946\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 521.2719 - val_loss: 119.5290\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 567.4512 - val_loss: 56.6683\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 501.4511 - val_loss: 215.3990\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 512.0624 - val_loss: 78.8501\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 465.5349 - val_loss: 176.7276\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 527.5615 - val_loss: 101.5187\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 584.9304 - val_loss: 468.4614\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 690.7010 - val_loss: 82.9497\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 517.2102 - val_loss: 124.6222\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 491.1400 - val_loss: 161.2238\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 561.1024 - val_loss: 95.8782\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 605.5038 - val_loss: 99.3381\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 567.7967 - val_loss: 128.4663\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 507.7787 - val_loss: 227.1314\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 481.7205 - val_loss: 98.9432\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 482.0020 - val_loss: 115.0571\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 484.2301 - val_loss: 322.6807\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 646.4953 - val_loss: 151.5944\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 526.0844 - val_loss: 108.6301\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 473.7589 - val_loss: 154.1161\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 497.6475 - val_loss: 157.7342\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 474.4660 - val_loss: 115.4949\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 472.3031 - val_loss: 226.1518\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 480.7190 - val_loss: 335.0706\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 544.6304 - val_loss: 124.9872\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 461.2526 - val_loss: 192.7345\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 463.7941 - val_loss: 204.8372\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 452.9304 - val_loss: 184.9131\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 442.4137 - val_loss: 272.6326\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 424.4930 - val_loss: 142.0423\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 434.1343 - val_loss: 150.3588\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 423.3055 - val_loss: 171.6583\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 421.7529 - val_loss: 146.8940\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 434.4107 - val_loss: 204.2228\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 465.5898 - val_loss: 240.6483\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 454.9568 - val_loss: 171.7506\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 425.1063 - val_loss: 165.8175\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 441.1367 - val_loss: 189.2680\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 433.9726 - val_loss: 190.4805\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 436.7383 - val_loss: 228.2485\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 434.4923 - val_loss: 377.3571\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 425.3661 - val_loss: 334.0051\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 428.0247 - val_loss: 301.1428\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 416.9190 - val_loss: 233.9558\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 429.6068 - val_loss: 201.8440\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 421.5763 - val_loss: 224.5294\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 528.8545 - val_loss: 269.2091\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 471.1458 - val_loss: 217.4499\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 376.9401 - val_loss: 238.3256\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 343.1308 - val_loss: 243.6980\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 402.2301 - val_loss: 331.7580\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 396.4330 - val_loss: 249.4644\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 415.6959 - val_loss: 273.5187\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 474.3031 - val_loss: 263.8872\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 401.5452 - val_loss: 297.0987\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 395.4341 - val_loss: 272.4544\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 404.2888 - val_loss: 515.6584\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 368.5068 - val_loss: 261.2606\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 349.9879 - val_loss: 293.5865\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 500.8591 - val_loss: 324.2357\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 396.7057 - val_loss: 275.8222\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 390.3640 - val_loss: 278.0972\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 537.5482 - val_loss: 287.8168\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 423.5482 - val_loss: 347.8526\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 405.9367 - val_loss: 518.1705\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 399.0343 - val_loss: 306.3737\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 463.5438 - val_loss: 321.2719\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 371.6494 - val_loss: 309.6451\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 351.0955 - val_loss: 312.7632\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 349.3908 - val_loss: 311.3179\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 386.1214 - val_loss: 437.6325\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 396.3234 - val_loss: 330.0799\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 371.7091 - val_loss: 347.6111\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 408.2406 - val_loss: 344.5854\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 344.6293 - val_loss: 355.0769\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 377.7714 - val_loss: 344.7240\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 370.5692 - val_loss: 364.0856\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 386.4226 - val_loss: 381.5345\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 382.2639 - val_loss: 410.9515\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 423.9337 - val_loss: 363.5861\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 462.0778 - val_loss: 517.1357\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 565.9222 - val_loss: 390.6565\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 422.8263 - val_loss: 653.4634\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 393.3074 - val_loss: 426.3470\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 411.5883 - val_loss: 415.1059\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 428.1960 - val_loss: 479.9992\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 355.9274 - val_loss: 406.3342\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 362.0845 - val_loss: 404.7575\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 450.5538 - val_loss: 428.7426\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 329.4910 - val_loss: 478.1944\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 369.5898 - val_loss: 519.8543\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 440.8849 - val_loss: 675.2622\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 481.6236 - val_loss: 568.8817\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 379.9821 - val_loss: 449.5765\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 342.0085 - val_loss: 440.5297\n",
            "RMSE:  20.988798844715348\n",
            "R2Score:  0.9468172692560104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nG9eqP3e2w1",
        "colab_type": "text"
      },
      "source": [
        "Save and Load the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp9QEBOCIkH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnnmodel.save(\"/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/0892691-CNN.h5\")"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e73hyXjmIpfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnnmodel=load_model(\"/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/0892691-CNN.h5\")"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60__HKXYfAhf",
        "colab_type": "text"
      },
      "source": [
        "Test the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK83f8tDI4cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5cc18509-a427-40ef-c484-b498766fcb78"
      },
      "source": [
        "#Test the model and calculate the performance metrics\n",
        "cnn_yPred = cnnmodel.predict(xTest)\n",
        "cnn_perf_metrics=showResults(yTest, cnn_yPred)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE:  37.43859771275274\n",
            "R2Score:  0.7754275142935152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8pHP9LNfJom",
        "colab_type": "text"
      },
      "source": [
        "Plot Actual value vs Predicted value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T2E2nkIJCWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ae562e03-7487-4863-a952-c3dd7b9157c8"
      },
      "source": [
        "##In ideal case if the actual value matches the predicted value \n",
        "#we would get a diagonal on the graph on mapping (actual,predicted) point\n",
        "#thus in the below graph we plot the diagonal \n",
        "#and then map the (actual,predicted) point to see the deviation\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(yTest, cnn_yPred,color='green')\n",
        "ax.plot([yTest.min(), yTest.max()], [yTest.min(), yTest.max()],'k--',color='blue',lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "#plt.show()\n",
        "plt.savefig('/content/drive/My Drive/Smart Health/Assignment_2/Assignment 2 - Question/CNN.png', format='png', dpi=1200)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFhkaAzUfbaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perf_metrics=pd.DataFrame({\"ANN\":[ann_perf_metrics[0],ann_perf_metrics[1]], \n",
        "                 \"CNN\":[cnn_perf_metrics[0],cnn_perf_metrics[1]]}) \n",
        "perf_metrics.index=['RMSE','R2Score']"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exTz3cKXiURB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "3c49e370-68e9-4b0b-848c-b1599bf31319"
      },
      "source": [
        "perf_metrics"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ANN</th>\n",
              "      <th>CNN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>RMSE</th>\n",
              "      <td>39.491309</td>\n",
              "      <td>37.438598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R2Score</th>\n",
              "      <td>0.750126</td>\n",
              "      <td>0.775428</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               ANN        CNN\n",
              "RMSE     39.491309  37.438598\n",
              "R2Score   0.750126   0.775428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    }
  ]
}